#blocked = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [32, 2], warpsPerCTA = [4, 1], order = [1, 0]}>
#loc = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0)
#loc1 = loc(unknown)
#loc16 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:27)
#loc20 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:26)
#loc24 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:70)
#loc39 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26)
#mma = #triton_gpu.amd_mfma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [2, 2], instrShape = [32, 32], isTransposed = true}>
#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0], hasLeadingOffset = false}>
#shared1 = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0], hasLeadingOffset = true}>
#shared2 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], hasLeadingOffset = true}>
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.shared = 33792 : i32, triton_gpu.target = "hip:gfx942", "triton_gpu.threads-per-warp" = 64 : i32} {
  tt.func public @matmul_persistent_ws_cooperative_kernel(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg6: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg7: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0), %arg8: i32 {tt.divisibility = 16 : i32} loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":30:0)) attributes {noinline = false} {
    %cst = arith.constant dense<0> : tensor<1xi32> loc(#loc1)
    %cst_0 = arith.constant dense<4> : tensor<1xi32> loc(#loc1)
    %c10_i32 = arith.constant 10 : i32 loc(#loc1)
    %c0 = arith.constant 0 : index loc(#loc1)
    %c64_i32 = arith.constant 64 : i32 loc(#loc1)
    %c-1_i32 = arith.constant -1 : i32 loc(#loc1)
    %c2_i64 = arith.constant 2 : i64 loc(#loc1)
    %true = arith.constant {async_task_id = dense<0> : vector<1xi32>} true loc(#loc1)
    %c1_i64 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 1 : i64 loc(#loc1)
    %c0_i64 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 0 : i64 loc(#loc1)
    %c2_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 2 : i32 loc(#loc1)
    %cst_1 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0.000000e+00> : tensor<128x256xf32, #mma> loc(#loc1)
    %cst_2 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0.000000e+00> : tensor<16x256xf16, #blocked> loc(#loc1)
    %cst_3 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0.000000e+00> : tensor<128x16xf16, #blocked1> loc(#loc1)
    %c255_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 255 : i32 loc(#loc1)
    %c4_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 4 : i32 loc(#loc1)
    %c256_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 256 : i32 loc(#loc1)
    %c16_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 16 : i32 loc(#loc1)
    %c15_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 15 : i32 loc(#loc1)
    %c1_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 1 : i32 loc(#loc1)
    %cst_4 = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<16> : tensor<128x16xi32, #blocked1> loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %0 = triton_gpu.local_alloc  {allocation.offset = 0 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %1 = triton_gpu.local_alloc  {allocation.offset = 16 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %2 = rocdl.workitem.id.x : i32 loc(#loc)
    %3 = arith.cmpi eq, %2, %c0_i32 : i32 loc(#loc)
    cf.cond_br %3, ^bb1, ^bb2 loc(#loc)
  ^bb1:  // pred: ^bb0
    %4 = triton_gpu.memdesc_subview %0[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %4 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %5 = triton_gpu.memdesc_subview %0[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %5 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %6 = triton_gpu.memdesc_subview %1[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %6 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %7 = triton_gpu.memdesc_subview %1[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %7 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %8 = triton_gpu.memdesc_subview %0[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %8 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %9 = triton_gpu.memdesc_subview %0[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %9 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %10 = triton_gpu.memdesc_subview %1[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %10 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %11 = triton_gpu.memdesc_subview %1[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %11 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.br ^bb2 loc(#loc)
  ^bb2:  // 2 preds: ^bb0, ^bb1
    gpu.barrier loc(#loc)
    %12 = triton_gpu.local_alloc  {allocation.offset = 32 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %13 = triton_gpu.local_alloc  {allocation.offset = 48 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.cond_br %3, ^bb3, ^bb4 loc(#loc)
  ^bb3:  // pred: ^bb2
    %14 = triton_gpu.memdesc_subview %12[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %14 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %15 = triton_gpu.memdesc_subview %12[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %15 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %16 = triton_gpu.memdesc_subview %13[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %16 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %17 = triton_gpu.memdesc_subview %13[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %17 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %18 = triton_gpu.memdesc_subview %12[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %18 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %19 = triton_gpu.memdesc_subview %12[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %19 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %20 = triton_gpu.memdesc_subview %13[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %20 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %21 = triton_gpu.memdesc_subview %13[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %21 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.br ^bb4 loc(#loc)
  ^bb4:  // 2 preds: ^bb2, ^bb3
    gpu.barrier loc(#loc)
    %22 = triton_gpu.local_alloc  {allocation.offset = 64 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %23 = triton_gpu.local_alloc  {allocation.offset = 80 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.cond_br %3, ^bb5, ^bb6 loc(#loc)
  ^bb5:  // pred: ^bb4
    %24 = triton_gpu.memdesc_subview %22[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %24 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %25 = triton_gpu.memdesc_subview %22[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %25 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %26 = triton_gpu.memdesc_subview %23[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %26 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %27 = triton_gpu.memdesc_subview %23[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %27 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %28 = triton_gpu.memdesc_subview %22[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %28 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %29 = triton_gpu.memdesc_subview %22[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %29 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %30 = triton_gpu.memdesc_subview %23[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %30 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %31 = triton_gpu.memdesc_subview %23[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %31 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.br ^bb6 loc(#loc)
  ^bb6:  // 2 preds: ^bb4, ^bb5
    gpu.barrier loc(#loc)
    %32 = triton_gpu.local_alloc  {allocation.offset = 96 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %33 = triton_gpu.local_alloc  {allocation.offset = 112 : i32} : () -> !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.cond_br %3, ^bb7, ^bb8 loc(#loc)
  ^bb7:  // pred: ^bb6
    %34 = triton_gpu.memdesc_subview %32[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %34 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %35 = triton_gpu.memdesc_subview %32[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %35 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %36 = triton_gpu.memdesc_subview %33[%c0_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %36 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %37 = triton_gpu.memdesc_subview %33[%c0_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %37 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %38 = triton_gpu.memdesc_subview %32[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %38 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %39 = triton_gpu.memdesc_subview %32[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %39 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %40 = triton_gpu.memdesc_subview %33[%c1_i32, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst_0, %40 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %41 = triton_gpu.memdesc_subview %33[%c1_i32, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_gpu.local_store %cst, %41 : tensor<1xi32> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    cf.br ^bb8 loc(#loc)
  ^bb8:  // 2 preds: ^bb6, ^bb7
    gpu.barrier loc(#loc)
    %42 = triton_gpu.local_alloc  {allocation.offset = 1024 : i32} : () -> !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %43 = triton_gpu.local_alloc  {allocation.offset = 9216 : i32} : () -> !tt.memdesc<2x16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc)
    %44 = triton_gpu.local_alloc  {allocation.offset = 25600 : i32} : () -> !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %45 = arith.divsi %2, %c64_i32 : i32 loc(#loc)
    %46 = arith.cmpi eq, %45, %c0_i32 : i32 loc(#loc)
    cf.cond_br %46, ^bb9, ^bb36 loc(#loc)
  ^bb9:  // pred: ^bb8
    %47 = arith.addi %arg3, %c255_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc59)
    %48 = arith.divsi %47, %c256_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc60)
    %49 = arith.addi %arg4, %c255_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc61)
    %50 = arith.divsi %49, %c256_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc62)
    %51 = arith.muli %48, %50 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc6)
    %52 = tt.get_program_id x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc7)
    %53 = tt.get_num_programs x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc8)
    %54 = arith.muli %50, %c4_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc9)
    %55 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc10)
    %56 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 256 : i32, start = 128 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc10)
    %57 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> loc(#loc10)
    %58 = tt.splat %arg3 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc11)
    %59 = tt.splat %arg4 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> loc(#loc12)
    %60 = tt.splat %arg6 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<128x1xi32, #blocked1> loc(#loc13)
    %61 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>> loc(#loc14)
    %62 = tt.expand_dims %61 {async_task_id = dense<0> : vector<1xi32>, axis = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x16xi32, #blocked1> loc(#loc14)
    %63 = tt.broadcast %62 {async_task_id = dense<0> : vector<1xi32>} : tensor<1x16xi32, #blocked1> -> tensor<128x16xi32, #blocked1> loc(#loc15)
    %64 = tt.splat %arg0 {async_task_id = dense<0> : vector<1xi32>} : !tt.ptr<f16> -> tensor<128x16x!tt.ptr<f16>, #blocked1> loc(#loc16)
    %65 = tt.make_range {async_task_id = dense<0> : vector<1xi32>, end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>> loc(#loc17)
    %66 = tt.expand_dims %65 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc17)
    %67 = tt.splat %arg7 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<16x1xi32, #blocked> loc(#loc18)
    %68 = arith.muli %66, %67 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x1xi32, #blocked> loc(#loc18)
    %69 = tt.broadcast %68 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x1xi32, #blocked> -> tensor<16x256xi32, #blocked> loc(#loc19)
    %70 = tt.splat %arg1 {async_task_id = dense<0> : vector<1xi32>} : !tt.ptr<f16> -> tensor<16x256x!tt.ptr<f16>, #blocked> loc(#loc20)
    %71 = arith.addi %arg5, %c15_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc63)
    %72 = arith.divsi %71, %c16_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc64)
    %73 = arith.muli %arg7, %c16_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
    %74 = tt.splat %73 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<16x256xi32, #blocked> loc(#loc23)
    cf.br ^bb10(%52, %c0_i64 : i32, i64) loc(#loc24)
  ^bb10(%75: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:70), %76: i64 loc(unknown)):  // 2 preds: ^bb9, ^bb34
    %77 = arith.cmpi slt, %75, %51 : i32 loc(#loc24)
    cf.cond_br %77, ^bb11, ^bb35 loc(#loc24)
  ^bb11:  // pred: ^bb10
    %78 = arith.divsi %75, %54 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc25)
    %79 = arith.muli %78, %c4_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc26)
    %80 = arith.subi %48, %79 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc27)
    %81 = arith.minsi %80, %c4_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc28)
    %82 = arith.remsi %75, %54 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc29)
    %83 = arith.remsi %82, %81 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc30)
    %84 = arith.addi %79, %83 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc31)
    %85 = arith.divsi %82, %81 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc32)
    %86 = arith.muli %84, %c256_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc33)
    %87 = tt.splat %86 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc34)
    %88 = arith.addi %87, %55 {async_task_id = dense<0> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc34)
    %89 = arith.addi %87, %56 {async_task_id = dense<0> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc34)
    %90 = arith.remsi %88, %58 {async_task_id = dense<0> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc11)
    %91 = arith.remsi %89, %58 {async_task_id = dense<0> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> loc(#loc11)
    %92 = arith.muli %85, %c256_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc35)
    %93 = tt.splat %92 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> loc(#loc36)
    %94 = arith.addi %93, %57 {async_task_id = dense<0> : vector<1xi32>} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> loc(#loc36)
    %95 = arith.remsi %94, %59 {async_task_id = dense<0> : vector<1xi32>} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> loc(#loc12)
    %96 = tt.expand_dims %90 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc37)
    %97 = tt.expand_dims %91 {async_task_id = dense<0> : vector<1xi32>, axis = 1 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc37)
    %98 = arith.muli %96, %60 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x1xi32, #blocked1> loc(#loc13)
    %99 = arith.muli %97, %60 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x1xi32, #blocked1> loc(#loc13)
    %100 = tt.broadcast %98 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x1xi32, #blocked1> -> tensor<128x16xi32, #blocked1> loc(#loc15)
    %101 = tt.broadcast %99 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x1xi32, #blocked1> -> tensor<128x16xi32, #blocked1> loc(#loc15)
    %102 = arith.addi %100, %63 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16xi32, #blocked1> loc(#loc15)
    %103 = arith.addi %101, %63 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16xi32, #blocked1> loc(#loc15)
    %104 = tt.addptr %64, %102 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16xi32, #blocked1> loc(#loc16)
    %105 = tt.addptr %64, %103 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16xi32, #blocked1> loc(#loc16)
    %106 = tt.expand_dims %95 {async_task_id = dense<0> : vector<1xi32>, axis = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> -> tensor<1x256xi32, #blocked> loc(#loc38)
    %107 = tt.broadcast %106 {async_task_id = dense<0> : vector<1xi32>} : tensor<1x256xi32, #blocked> -> tensor<16x256xi32, #blocked> loc(#loc19)
    %108 = arith.addi %69, %107 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x256xi32, #blocked> loc(#loc19)
    %109 = tt.addptr %70, %108 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x256x!tt.ptr<f16>, #blocked>, tensor<16x256xi32, #blocked> loc(#loc20)
    %110 = arith.divui %76, %c2_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %111 = arith.muli %110, %c2_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %112 = arith.subi %76, %111 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %113 = arith.trunci %112 {async_task_id = dense<0> : vector<1xi32>} : i64 to i32 loc(#loc39)
    %114 = arith.andi %110, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %115 = arith.trunci %114 {async_task_id = dense<0> : vector<1xi32>} : i64 to i1 loc(#loc39)
    cf.br ^bb12(%c0_i32, %109, %104, %105, %115, %113 : i32, tensor<16x256x!tt.ptr<f16>, #blocked>, tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16x!tt.ptr<f16>, #blocked1>, i1, i32) loc(#loc39)
  ^bb12(%116: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %117: tensor<16x256x!tt.ptr<f16>, #blocked> loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:26), %118: tensor<128x16x!tt.ptr<f16>, #blocked1> loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:27), %119: tensor<128x16x!tt.ptr<f16>, #blocked1> loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:27), %120: i1 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %121: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26)):  // 2 preds: ^bb11, ^bb33
    %122 = arith.cmpi slt, %116, %72 : i32 loc(#loc39)
    cf.cond_br %122, ^bb13, ^bb34 loc(#loc39)
  ^bb13:  // pred: ^bb12
    %123 = arith.muli %116, %c16_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc40)
    %124 = arith.subi %arg5, %123 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc41)
    %125 = tt.splat %124 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<1x16xi32, #blocked1> loc(#loc42)
    %126 = arith.cmpi slt, %62, %125 {async_task_id = dense<0> : vector<1xi32>} : tensor<1x16xi32, #blocked1> loc(#loc42)
    %127 = tt.broadcast %126 {async_task_id = dense<0> : vector<1xi32>} : tensor<1x16xi1, #blocked1> -> tensor<128x16xi1, #blocked1> loc(#loc43)
    %128 = triton_gpu.memdesc_subview %1[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %129 = arith.xori %120, %true : i1 loc(#loc43)
    %130 = arith.extui %129 : i1 to i32 loc(#loc43)
    cf.br ^bb14 loc(#loc43)
  ^bb14:  // 2 preds: ^bb13, ^bb15
    %131 = triton_gpu.local_load %128 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc43)
    %extracted = tensor.extract %131[%c0] : tensor<1xi32> loc(#loc43)
    %132 = arith.cmpi eq, %extracted, %130 : i32 loc(#loc43)
    cf.cond_br %132, ^bb15, ^bb16 loc(#loc43)
  ^bb15:  // pred: ^bb14
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc43)
    cf.br ^bb14 loc(#loc43)
  ^bb16:  // pred: ^bb14
    %133 = tt.load %118, %127, %cst_3 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1> loc(#loc43)
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc43)
    %134 = triton_gpu.memdesc_subview %44[%121, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    triton_gpu.local_store %133, %134 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16xf16, #blocked1> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %135 = triton_gpu.memdesc_subview %0[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %136 = triton_gpu.memdesc_subview %0[%121, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %137 = arith.remsi %2, %c64_i32 : i32 loc(#loc43)
    %138 = arith.cmpi eq, %137, %c0_i32 : i32 loc(#loc43)
    cf.cond_br %138, ^bb17, ^bb18 loc(#loc43)
  ^bb17:  // pred: ^bb16
    amdgpu.arrive_barrier %136, %135 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    cf.br ^bb18 loc(#loc43)
  ^bb18:  // 2 preds: ^bb16, ^bb17
    %139 = tt.splat %124 {async_task_id = dense<0> : vector<1xi32>} : i32 -> tensor<16x1xi32, #blocked> loc(#loc44)
    %140 = arith.cmpi slt, %66, %139 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x1xi32, #blocked> loc(#loc44)
    %141 = tt.broadcast %140 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x1xi1, #blocked> -> tensor<16x256xi1, #blocked> loc(#loc45)
    %142 = triton_gpu.memdesc_subview %33[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb19 loc(#loc45)
  ^bb19:  // 2 preds: ^bb18, ^bb20
    %143 = triton_gpu.local_load %142 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc45)
    %extracted_5 = tensor.extract %143[%c0] : tensor<1xi32> loc(#loc45)
    %144 = arith.cmpi eq, %extracted_5, %130 : i32 loc(#loc45)
    cf.cond_br %144, ^bb20, ^bb21 loc(#loc45)
  ^bb20:  // pred: ^bb19
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc45)
    cf.br ^bb19 loc(#loc45)
  ^bb21:  // pred: ^bb19
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc45)
    %145 = triton_gpu.memdesc_subview %23[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb22 loc(#loc45)
  ^bb22:  // 2 preds: ^bb21, ^bb23
    %146 = triton_gpu.local_load %145 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc45)
    %extracted_6 = tensor.extract %146[%c0] : tensor<1xi32> loc(#loc45)
    %147 = arith.cmpi eq, %extracted_6, %130 : i32 loc(#loc45)
    cf.cond_br %147, ^bb23, ^bb24 loc(#loc45)
  ^bb23:  // pred: ^bb22
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc45)
    cf.br ^bb22 loc(#loc45)
  ^bb24:  // pred: ^bb22
    %148 = tt.load %117, %141, %cst_2 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x256x!tt.ptr<f16>, #blocked> loc(#loc45)
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc45)
    %149 = triton_gpu.memdesc_subview %43[%121, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<2x16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc45)
    triton_gpu.local_store %148, %149 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x256xf16, #blocked> -> !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc45)
    %150 = triton_gpu.memdesc_subview %22[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    %151 = triton_gpu.memdesc_subview %22[%121, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.cond_br %138, ^bb25, ^bb26 loc(#loc45)
  ^bb25:  // pred: ^bb24
    amdgpu.arrive_barrier %151, %150 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb26 loc(#loc45)
  ^bb26:  // 2 preds: ^bb24, ^bb25
    %152 = triton_gpu.memdesc_subview %32[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    %153 = triton_gpu.memdesc_subview %32[%121, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.cond_br %138, ^bb27, ^bb28 loc(#loc45)
  ^bb27:  // pred: ^bb26
    amdgpu.arrive_barrier %153, %152 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb28 loc(#loc45)
  ^bb28:  // 2 preds: ^bb26, ^bb27
    %154 = triton_gpu.memdesc_subview %13[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    cf.br ^bb29 loc(#loc43)
  ^bb29:  // 2 preds: ^bb28, ^bb30
    %155 = triton_gpu.local_load %154 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc43)
    %extracted_7 = tensor.extract %155[%c0] : tensor<1xi32> loc(#loc43)
    %156 = arith.cmpi eq, %extracted_7, %130 : i32 loc(#loc43)
    cf.cond_br %156, ^bb30, ^bb31 loc(#loc43)
  ^bb30:  // pred: ^bb29
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc43)
    cf.br ^bb29 loc(#loc43)
  ^bb31:  // pred: ^bb29
    %157 = tt.load %119, %127, %cst_3 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1> loc(#loc43)
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc43)
    %158 = triton_gpu.memdesc_subview %42[%121, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    triton_gpu.local_store %157, %158 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16xf16, #blocked1> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %159 = triton_gpu.memdesc_subview %12[%121, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %160 = triton_gpu.memdesc_subview %12[%121, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    cf.cond_br %138, ^bb32, ^bb33 loc(#loc43)
  ^bb32:  // pred: ^bb31
    amdgpu.arrive_barrier %160, %159 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    cf.br ^bb33 loc(#loc43)
  ^bb33:  // 2 preds: ^bb31, ^bb32
    %161 = tt.addptr %118, %cst_4 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16xi32, #blocked1> loc(#loc46)
    %162 = tt.addptr %119, %cst_4 {async_task_id = dense<0> : vector<1xi32>} : tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16xi32, #blocked1> loc(#loc46)
    %163 = tt.addptr %117, %74 {async_task_id = dense<0> : vector<1xi32>} : tensor<16x256x!tt.ptr<f16>, #blocked>, tensor<16x256xi32, #blocked> loc(#loc23)
    %164 = arith.addi %121, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc39)
    %165 = arith.cmpi uge, %164, %c2_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc39)
    %166 = arith.cmpi ult, %164, %c2_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc39)
    %167 = arith.addi %121, %c-1_i32 : i32 loc(#loc39)
    %168 = arith.select %165, %167, %164 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc39)
    %169 = arith.xori %120, %true {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc39)
    %170 = arith.andi %165, %169 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc39)
    %171 = arith.andi %166, %120 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc39)
    %172 = arith.ori %170, %171 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc39)
    %173 = arith.addi %116, %c1_i32 : i32 loc(#loc39)
    cf.br ^bb12(%173, %163, %161, %162, %172, %168 : i32, tensor<16x256x!tt.ptr<f16>, #blocked>, tensor<128x16x!tt.ptr<f16>, #blocked1>, tensor<128x16x!tt.ptr<f16>, #blocked1>, i1, i32) loc(#loc39)
  ^bb34:  // pred: ^bb12
    %174 = arith.addi %72, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc39)
    %175 = arith.extsi %174 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc39)
    %176 = arith.subi %175, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %177 = arith.addi %76, %176 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc39)
    %178 = arith.addi %75, %53 : i32 loc(#loc24)
    cf.br ^bb10(%178, %177 : i32, i64) loc(#loc24)
  ^bb35:  // pred: ^bb10
    cf.br ^bb36 loc(#loc)
  ^bb36:  // 2 preds: ^bb8, ^bb35
    %179 = arith.cmpi eq, %45, %c1_i32 : i32 loc(#loc)
    cf.cond_br %179, ^bb37, ^bb54 loc(#loc)
  ^bb37:  // pred: ^bb36
    %180 = arith.addi %arg3, %c255_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc59)
    %181 = arith.divsi %180, %c256_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc60)
    %182 = arith.addi %arg4, %c255_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc61)
    %183 = arith.divsi %182, %c256_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc62)
    %184 = arith.muli %181, %183 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc6)
    %185 = tt.get_program_id x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc7)
    %186 = tt.get_num_programs x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc8)
    %187 = arith.muli %183, %c4_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc9)
    %188 = tt.make_range {async_task_id = dense<1> : vector<1xi32>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc10)
    %189 = tt.make_range {async_task_id = dense<1> : vector<1xi32>, end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc10)
    %190 = arith.addi %arg5, %c15_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc63)
    %191 = arith.divsi %190, %c16_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc64)
    %192 = tt.splat %arg8 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<128x1xi32, #mma> loc(#loc47)
    %193 = tt.splat %arg2 {async_task_id = dense<1> : vector<1xi32>} : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #mma> loc(#loc48)
    %194 = tt.splat %arg3 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<128x1xi32, #mma> loc(#loc49)
    %195 = tt.splat %arg4 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<1x256xi32, #mma> loc(#loc50)
    cf.br ^bb38(%185, %c0_i64 : i32, i64) loc(#loc24)
  ^bb38(%196: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:70), %197: i64 loc(unknown)):  // 2 preds: ^bb37, ^bb52
    %198 = arith.cmpi slt, %196, %184 : i32 loc(#loc24)
    cf.cond_br %198, ^bb39, ^bb53 loc(#loc24)
  ^bb39:  // pred: ^bb38
    %199 = arith.divsi %196, %187 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc25)
    %200 = arith.muli %199, %c4_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc26)
    %201 = arith.subi %181, %200 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc27)
    %202 = arith.minsi %201, %c4_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc28)
    %203 = arith.remsi %196, %187 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc29)
    %204 = arith.remsi %203, %202 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc30)
    %205 = arith.addi %200, %204 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc31)
    %206 = arith.divsi %203, %202 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc32)
    %207 = arith.muli %205, %c256_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc33)
    %208 = tt.splat %207 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc34)
    %209 = arith.addi %208, %188 {async_task_id = dense<1> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc34)
    %210 = arith.muli %206, %c256_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc35)
    %211 = tt.splat %210 {async_task_id = dense<1> : vector<1xi32>} : i32 -> tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc36)
    %212 = arith.addi %211, %189 {async_task_id = dense<1> : vector<1xi32>} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc36)
    %213 = arith.divui %197, %c2_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %214 = arith.muli %213, %c2_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %215 = arith.subi %197, %214 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %216 = arith.trunci %215 {async_task_id = dense<1> : vector<1xi32>} : i64 to i32 loc(#loc39)
    %217 = arith.andi %213, %c1_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %218 = arith.trunci %217 {async_task_id = dense<1> : vector<1xi32>} : i64 to i1 loc(#loc39)
    cf.br ^bb40(%c0_i32, %cst_1, %218, %216 : i32, tensor<128x256xf32, #mma>, i1, i32) loc(#loc39)
  ^bb40(%219: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %220: tensor<128x256xf32, #mma> loc(unknown), %221: i1 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %222: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26)):  // 2 preds: ^bb39, ^bb51
    %223 = arith.cmpi slt, %219, %191 : i32 loc(#loc39)
    cf.cond_br %223, ^bb41, ^bb52 loc(#loc39)
  ^bb41:  // pred: ^bb40
    %224 = triton_gpu.memdesc_subview %1[%222, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %225 = arith.extui %221 : i1 to i32 loc(#loc43)
    cf.br ^bb42 loc(#loc43)
  ^bb42:  // 2 preds: ^bb41, ^bb43
    %226 = triton_gpu.local_load %224 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc43)
    %extracted_8 = tensor.extract %226[%c0] : tensor<1xi32> loc(#loc43)
    %227 = arith.cmpi eq, %extracted_8, %225 : i32 loc(#loc43)
    cf.cond_br %227, ^bb43, ^bb44 loc(#loc43)
  ^bb43:  // pred: ^bb42
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc43)
    cf.br ^bb42 loc(#loc43)
  ^bb44:  // pred: ^bb42
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc43)
    %228 = triton_gpu.memdesc_subview %44[%222, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %229 = triton_gpu.local_load %228 : !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> tensor<128x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc43)
    %230 = triton_gpu.memdesc_subview %23[%222, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb45 loc(#loc45)
  ^bb45:  // 2 preds: ^bb44, ^bb46
    %231 = triton_gpu.local_load %230 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc45)
    %extracted_9 = tensor.extract %231[%c0] : tensor<1xi32> loc(#loc45)
    %232 = arith.cmpi eq, %extracted_9, %225 : i32 loc(#loc45)
    cf.cond_br %232, ^bb46, ^bb47 loc(#loc45)
  ^bb46:  // pred: ^bb45
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc45)
    cf.br ^bb45 loc(#loc45)
  ^bb47:  // pred: ^bb45
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc45)
    %233 = triton_gpu.memdesc_subview %43[%222, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<2x16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc45)
    %234 = triton_gpu.local_load %233 : !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> -> tensor<16x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc45)
    %235 = tt.dot %229, %234, %220 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x256xf32, #mma> loc(#loc51)
    %236 = triton_gpu.memdesc_subview %1[%222, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    %237 = arith.remsi %2, %c64_i32 : i32 loc(#loc51)
    %238 = arith.cmpi eq, %237, %c0_i32 : i32 loc(#loc51)
    cf.cond_br %238, ^bb48, ^bb49 loc(#loc51)
  ^bb48:  // pred: ^bb47
    amdgpu.arrive_barrier %236, %224 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.br ^bb49 loc(#loc51)
  ^bb49:  // 2 preds: ^bb47, ^bb48
    %239 = triton_gpu.memdesc_subview %23[%222, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.cond_br %238, ^bb50, ^bb51 loc(#loc51)
  ^bb50:  // pred: ^bb49
    amdgpu.arrive_barrier %239, %230 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.br ^bb51 loc(#loc51)
  ^bb51:  // 2 preds: ^bb49, ^bb50
    %240 = arith.addi %222, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc39)
    %241 = arith.cmpi uge, %240, %c2_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc39)
    %242 = arith.cmpi ult, %240, %c2_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc39)
    %243 = arith.addi %222, %c-1_i32 : i32 loc(#loc39)
    %244 = arith.select %241, %243, %240 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc39)
    %245 = arith.xori %221, %true {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc39)
    %246 = arith.andi %241, %245 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc39)
    %247 = arith.andi %242, %221 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc39)
    %248 = arith.ori %246, %247 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc39)
    %249 = arith.addi %219, %c1_i32 : i32 loc(#loc39)
    cf.br ^bb40(%249, %235, %248, %244 : i32, tensor<128x256xf32, #mma>, i1, i32) loc(#loc39)
  ^bb52:  // pred: ^bb40
    %250 = arith.addi %191, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc39)
    %251 = arith.extsi %250 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc39)
    %252 = arith.subi %251, %c1_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %253 = arith.addi %197, %252 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc39)
    %254 = arith.truncf %220 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x256xf32, #mma> to tensor<128x256xf16, #mma> loc(#loc52)
    %255 = tt.expand_dims %209 {async_task_id = dense<1> : vector<1xi32>, axis = 1 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi32, #mma> loc(#loc53)
    %256 = arith.muli %192, %255 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x1xi32, #mma> loc(#loc47)
    %257 = tt.addptr %193, %256 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x1x!tt.ptr<f16>, #mma>, tensor<128x1xi32, #mma> loc(#loc48)
    %258 = tt.expand_dims %212 {async_task_id = dense<1> : vector<1xi32>, axis = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> -> tensor<1x256xi32, #mma> loc(#loc54)
    %259 = tt.broadcast %257 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x1x!tt.ptr<f16>, #mma> -> tensor<128x256x!tt.ptr<f16>, #mma> loc(#loc55)
    %260 = tt.broadcast %258 {async_task_id = dense<1> : vector<1xi32>} : tensor<1x256xi32, #mma> -> tensor<128x256xi32, #mma> loc(#loc55)
    %261 = tt.addptr %259, %260 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x256x!tt.ptr<f16>, #mma>, tensor<128x256xi32, #mma> loc(#loc55)
    %262 = arith.cmpi slt, %255, %194 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x1xi32, #mma> loc(#loc49)
    %263 = arith.cmpi slt, %258, %195 {async_task_id = dense<1> : vector<1xi32>} : tensor<1x256xi32, #mma> loc(#loc50)
    %264 = tt.broadcast %262 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x1xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc56)
    %265 = tt.broadcast %263 {async_task_id = dense<1> : vector<1xi32>} : tensor<1x256xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc56)
    %266 = arith.andi %264, %265 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x256xi1, #mma> loc(#loc56)
    tt.store %261, %254, %266 {async_task_id = dense<1> : vector<1xi32>} : tensor<128x256x!tt.ptr<f16>, #mma> loc(#loc57)
    %267 = arith.addi %196, %186 : i32 loc(#loc24)
    cf.br ^bb38(%267, %253 : i32, i64) loc(#loc24)
  ^bb53:  // pred: ^bb38
    cf.br ^bb54 loc(#loc)
  ^bb54:  // 2 preds: ^bb36, ^bb53
    %268 = arith.cmpi eq, %45, %c2_i32 : i32 loc(#loc)
    cf.cond_br %268, ^bb55, ^bb72 loc(#loc)
  ^bb55:  // pred: ^bb54
    %269 = arith.addi %arg3, %c255_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc59)
    %270 = arith.divsi %269, %c256_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc60)
    %271 = arith.addi %arg4, %c255_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc61)
    %272 = arith.divsi %271, %c256_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc62)
    %273 = arith.muli %270, %272 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc6)
    %274 = tt.get_program_id x {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc7)
    %275 = tt.get_num_programs x {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc8)
    %276 = arith.muli %272, %c4_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc9)
    %277 = tt.make_range {async_task_id = dense<2> : vector<1xi32>, end = 256 : i32, start = 128 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc10)
    %278 = tt.make_range {async_task_id = dense<2> : vector<1xi32>, end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc10)
    %279 = arith.addi %arg5, %c15_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc63)
    %280 = arith.divsi %279, %c16_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc64)
    %281 = tt.splat %arg8 {async_task_id = dense<2> : vector<1xi32>} : i32 -> tensor<128x1xi32, #mma> loc(#loc47)
    %282 = tt.splat %arg2 {async_task_id = dense<2> : vector<1xi32>} : !tt.ptr<f16> -> tensor<128x1x!tt.ptr<f16>, #mma> loc(#loc48)
    %283 = tt.splat %arg3 {async_task_id = dense<2> : vector<1xi32>} : i32 -> tensor<128x1xi32, #mma> loc(#loc49)
    %284 = tt.splat %arg4 {async_task_id = dense<2> : vector<1xi32>} : i32 -> tensor<1x256xi32, #mma> loc(#loc50)
    cf.br ^bb56(%274, %c0_i64 : i32, i64) loc(#loc24)
  ^bb56(%285: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:70), %286: i64 loc(unknown)):  // 2 preds: ^bb55, ^bb70
    %287 = arith.cmpi slt, %285, %273 : i32 loc(#loc24)
    cf.cond_br %287, ^bb57, ^bb71 loc(#loc24)
  ^bb57:  // pred: ^bb56
    %288 = arith.divsi %285, %276 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc25)
    %289 = arith.muli %288, %c4_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc26)
    %290 = arith.subi %270, %289 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc27)
    %291 = arith.minsi %290, %c4_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc28)
    %292 = arith.remsi %285, %276 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc29)
    %293 = arith.remsi %292, %291 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc30)
    %294 = arith.addi %289, %293 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc31)
    %295 = arith.divsi %292, %291 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc32)
    %296 = arith.muli %294, %c256_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc33)
    %297 = tt.splat %296 {async_task_id = dense<2> : vector<1xi32>} : i32 -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc34)
    %298 = arith.addi %297, %277 {async_task_id = dense<2> : vector<1xi32>} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc34)
    %299 = arith.muli %295, %c256_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc35)
    %300 = tt.splat %299 {async_task_id = dense<2> : vector<1xi32>} : i32 -> tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc36)
    %301 = arith.addi %300, %278 {async_task_id = dense<2> : vector<1xi32>} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> loc(#loc36)
    %302 = arith.divui %286, %c2_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %303 = arith.muli %302, %c2_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %304 = arith.subi %286, %303 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %305 = arith.trunci %304 {async_task_id = dense<2> : vector<1xi32>} : i64 to i32 loc(#loc39)
    %306 = arith.andi %302, %c1_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %307 = arith.trunci %306 {async_task_id = dense<2> : vector<1xi32>} : i64 to i1 loc(#loc39)
    cf.br ^bb58(%c0_i32, %cst_1, %307, %305 : i32, tensor<128x256xf32, #mma>, i1, i32) loc(#loc39)
  ^bb58(%308: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %309: tensor<128x256xf32, #mma> loc(unknown), %310: i1 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26), %311: i32 loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:26)):  // 2 preds: ^bb57, ^bb69
    %312 = arith.cmpi slt, %308, %280 : i32 loc(#loc39)
    cf.cond_br %312, ^bb59, ^bb70 loc(#loc39)
  ^bb59:  // pred: ^bb58
    %313 = triton_gpu.memdesc_subview %13[%311, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %314 = arith.extui %310 : i1 to i32 loc(#loc43)
    cf.br ^bb60 loc(#loc43)
  ^bb60:  // 2 preds: ^bb59, ^bb61
    %315 = triton_gpu.local_load %313 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc43)
    %extracted_10 = tensor.extract %315[%c0] : tensor<1xi32> loc(#loc43)
    %316 = arith.cmpi eq, %extracted_10, %314 : i32 loc(#loc43)
    cf.cond_br %316, ^bb61, ^bb62 loc(#loc43)
  ^bb61:  // pred: ^bb60
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc43)
    cf.br ^bb60 loc(#loc43)
  ^bb62:  // pred: ^bb60
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc43)
    %317 = triton_gpu.memdesc_subview %42[%311, %c0_i32, %c0_i32] {async_task_id = dense<2> : vector<1xi32>} : !tt.memdesc<2x128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc43)
    %318 = triton_gpu.local_load %317 : !tt.memdesc<128x16xf16, #shared1, #triton_gpu.shared_memory, mutable> -> tensor<128x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> loc(#loc43)
    %319 = triton_gpu.memdesc_subview %33[%311, %c1_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc45)
    cf.br ^bb63 loc(#loc45)
  ^bb63:  // 2 preds: ^bb62, ^bb64
    %320 = triton_gpu.local_load %319 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> -> tensor<1xi32> loc(#loc45)
    %extracted_11 = tensor.extract %320[%c0] : tensor<1xi32> loc(#loc45)
    %321 = arith.cmpi eq, %extracted_11, %314 : i32 loc(#loc45)
    cf.cond_br %321, ^bb64, ^bb65 loc(#loc45)
  ^bb64:  // pred: ^bb63
    llvm.call_intrinsic "llvm.amdgcn.s.sleep"(%c10_i32) : (i32) -> () loc(#loc45)
    cf.br ^bb63 loc(#loc45)
  ^bb65:  // pred: ^bb63
    llvm.call_intrinsic "llvm.amdgcn.s.wakeup"() : () -> () loc(#loc45)
    %322 = triton_gpu.memdesc_subview %43[%311, %c0_i32, %c0_i32] {async_task_id = dense<2> : vector<1xi32>} : !tt.memdesc<2x16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc45)
    %323 = triton_gpu.local_load %322 : !tt.memdesc<16x256xf16, #shared2, #triton_gpu.shared_memory, mutable> -> tensor<16x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> loc(#loc45)
    %324 = tt.dot %318, %323, %309 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x256xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<128x256xf32, #mma> loc(#loc51)
    %325 = triton_gpu.memdesc_subview %33[%311, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    %326 = arith.remsi %2, %c64_i32 : i32 loc(#loc51)
    %327 = arith.cmpi eq, %326, %c0_i32 : i32 loc(#loc51)
    cf.cond_br %327, ^bb66, ^bb67 loc(#loc51)
  ^bb66:  // pred: ^bb65
    amdgpu.arrive_barrier %325, %319 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.br ^bb67 loc(#loc51)
  ^bb67:  // 2 preds: ^bb65, ^bb66
    %328 = triton_gpu.memdesc_subview %13[%311, %c0_i32] : !tt.memdesc<2x2xi32, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.cond_br %327, ^bb68, ^bb69 loc(#loc51)
  ^bb68:  // pred: ^bb67
    amdgpu.arrive_barrier %328, %313 : !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable>, !tt.memdesc<1xi32, #shared, #triton_gpu.shared_memory, mutable> loc(#loc51)
    cf.br ^bb69 loc(#loc51)
  ^bb69:  // 2 preds: ^bb67, ^bb68
    %329 = arith.addi %311, %c1_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc39)
    %330 = arith.cmpi uge, %329, %c2_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc39)
    %331 = arith.cmpi ult, %329, %c2_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc39)
    %332 = arith.addi %311, %c-1_i32 : i32 loc(#loc39)
    %333 = arith.select %330, %332, %329 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc39)
    %334 = arith.xori %310, %true {async_task_id = dense<2> : vector<1xi32>} : i1 loc(#loc39)
    %335 = arith.andi %330, %334 {async_task_id = dense<2> : vector<1xi32>} : i1 loc(#loc39)
    %336 = arith.andi %331, %310 {async_task_id = dense<2> : vector<1xi32>} : i1 loc(#loc39)
    %337 = arith.ori %335, %336 {async_task_id = dense<2> : vector<1xi32>} : i1 loc(#loc39)
    %338 = arith.addi %308, %c1_i32 : i32 loc(#loc39)
    cf.br ^bb58(%338, %324, %337, %333 : i32, tensor<128x256xf32, #mma>, i1, i32) loc(#loc39)
  ^bb70:  // pred: ^bb58
    %339 = arith.addi %280, %c1_i32 {async_task_id = dense<2> : vector<1xi32>} : i32 loc(#loc39)
    %340 = arith.extsi %339 {async_task_id = dense<2> : vector<1xi32>} : i32 to i64 loc(#loc39)
    %341 = arith.subi %340, %c1_i64 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %342 = arith.addi %286, %341 {async_task_id = dense<2> : vector<1xi32>} : i64 loc(#loc39)
    %343 = arith.truncf %309 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x256xf32, #mma> to tensor<128x256xf16, #mma> loc(#loc52)
    %344 = tt.expand_dims %298 {async_task_id = dense<2> : vector<1xi32>, axis = 1 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xi32, #mma> loc(#loc53)
    %345 = arith.muli %281, %344 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x1xi32, #mma> loc(#loc47)
    %346 = tt.addptr %282, %345 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x1x!tt.ptr<f16>, #mma>, tensor<128x1xi32, #mma> loc(#loc48)
    %347 = tt.expand_dims %301 {async_task_id = dense<2> : vector<1xi32>, axis = 0 : i32} : tensor<256xi32, #triton_gpu.slice<{dim = 0, parent = #mma}>> -> tensor<1x256xi32, #mma> loc(#loc54)
    %348 = tt.broadcast %346 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x1x!tt.ptr<f16>, #mma> -> tensor<128x256x!tt.ptr<f16>, #mma> loc(#loc55)
    %349 = tt.broadcast %347 {async_task_id = dense<2> : vector<1xi32>} : tensor<1x256xi32, #mma> -> tensor<128x256xi32, #mma> loc(#loc55)
    %350 = tt.addptr %348, %349 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x256x!tt.ptr<f16>, #mma>, tensor<128x256xi32, #mma> loc(#loc55)
    %351 = arith.cmpi slt, %344, %283 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x1xi32, #mma> loc(#loc49)
    %352 = arith.cmpi slt, %347, %284 {async_task_id = dense<2> : vector<1xi32>} : tensor<1x256xi32, #mma> loc(#loc50)
    %353 = tt.broadcast %351 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x1xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc56)
    %354 = tt.broadcast %352 {async_task_id = dense<2> : vector<1xi32>} : tensor<1x256xi1, #mma> -> tensor<128x256xi1, #mma> loc(#loc56)
    %355 = arith.andi %353, %354 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x256xi1, #mma> loc(#loc56)
    tt.store %350, %343, %355 {async_task_id = dense<2> : vector<1xi32>} : tensor<128x256x!tt.ptr<f16>, #mma> loc(#loc57)
    %356 = arith.addi %285, %275 : i32 loc(#loc24)
    cf.br ^bb56(%356, %342 : i32, i64) loc(#loc24)
  ^bb71:  // pred: ^bb56
    cf.br ^bb72 loc(#loc)
  ^bb72:  // 2 preds: ^bb54, ^bb71
    tt.return loc(#loc58)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/home/kmanivannan/fb-experimental/python/triton/language/standard.py":40:22)
#loc3 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":59:27)
#loc4 = loc("/home/kmanivannan/fb-experimental/python/triton/language/standard.py":40:28)
#loc5 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":59:54)
#loc6 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":59:43)
#loc7 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:35)
#loc8 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:66)
#loc9 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":69:42)
#loc10 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":83:56)
#loc11 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":83:73)
#loc12 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":84:72)
#loc13 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:47)
#loc14 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:66)
#loc15 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:59)
#loc17 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:33)
#loc18 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:44)
#loc19 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:56)
#loc21 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":90:37)
#loc22 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":100:37)
#loc23 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":100:22)
#loc25 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":70:26)
#loc26 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":71:33)
#loc27 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":72:39)
#loc28 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":72:52)
#loc29 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":73:38)
#loc30 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":73:58)
#loc31 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":73:31)
#loc32 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":74:44)
#loc33 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":83:28)
#loc34 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":83:43)
#loc35 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":84:27)
#loc36 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":84:42)
#loc37 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":86:36)
#loc38 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":87:64)
#loc40 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":92:56)
#loc41 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":92:52)
#loc42 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":92:48)
#loc43 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":92:16)
#loc44 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":95:47)
#loc45 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":95:16)
#loc46 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":99:23)
#loc47 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":106:38)
#loc48 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":106:26)
#loc49 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":107:39)
#loc50 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":107:64)
#loc51 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":98:39)
#loc52 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":102:29)
#loc53 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":106:47)
#loc54 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":106:78)
#loc55 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":106:58)
#loc56 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":107:45)
#loc57 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":108:26)
#loc58 = loc("/home/kmanivannan/fb-experimental/python/gemmbench/impls/matmul_persistent_ws_cooperative.py":60:4)
#loc59 = loc(callsite(#loc2 at #loc3))
#loc60 = loc(callsite(#loc4 at #loc3))
#loc61 = loc(callsite(#loc2 at #loc5))
#loc62 = loc(callsite(#loc4 at #loc5))
#loc63 = loc(callsite(#loc2 at #loc21))
#loc64 = loc(callsite(#loc4 at #loc21))
