// RUN: triton-opt %s --nvgpu-test-ws-memory-planner=num-buffers=3 2>&1 | FileCheck %s

// Test case: FA FWD pattern using allocateTMemAllocs2 algorithm.
// This test verifies the TMEM buffer allocation for Flash Attention forward pass.
//
// The key buffers in allocation order (12 total):
//   [0] acc0: liveness=[78-215) size=128x128 - accumulator, long-lived
//   [1] acc1: liveness=[79-236) size=128x128 - accumulator, long-lived
//   [2] qk0: liveness=[85-88) size=128x128 - temp buffer, short-lived
//   [3] qk1: liveness=[86-89) size=128x128 - temp buffer, short-lived
//   [4] acc_scaled0: liveness=[165-174) size=128x64 - intermediate
//   [5] acc_scaled1: liveness=[168-175) size=128x64 - intermediate
//   [6] alpha0: liveness=[115-140) size=128x1 - intermediate
//   [7] alpha1: liveness=[120-144) size=128x1 - intermediate
//   [8-11] offsetkv_y: liveness=[185-226) size=128x1 - outside loop
//
// Expected behavior:
//   - acc_scaled0/1 reuse qk0/qk1 at col 0
//   - alpha0/1 reuse qk0/qk1 at col 64 (after acc_scaled)
//   - offsetkv_y buffers reuse qk0/qk1 at cols 64-67

// CHECK-LABEL: tt.func public @_attn_fwd
// Verify that the pass completes successfully (no errors)
// CHECK-NOT: error

// -----// WarpSpec internal IR Dump After: doBufferAllocation
#blocked = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 2, 64], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 64, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#blocked6 = #ttg.blocked<{sizePerThread = [1, 64], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 128, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [2, 0, 1]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 2, 128], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [1, 0, 2]}>
#linear = #ttg.linear<{register = [], lane = [[1], [2], [4], [8], [16]], warp = [[32], [64]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1, 0], [0, 0, 1], [0, 0, 2], [0, 0, 4], [0, 0, 8], [0, 0, 16], [0, 0, 32], [128, 0, 0]], lane = [[1, 0, 0], [2, 0, 0], [4, 0, 0], [8, 0, 0], [16, 0, 0]], warp = [[32, 0, 0], [64, 0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 64], [0, 1], [0, 2], [0, 4], [0, 8], [0, 16], [0, 32], [128, 0]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0]], warp = [[32, 0], [64, 0]], block = []}>
#loc = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":582:0)
#loc2 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":671:12)
#loc4 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":194:12)
#loc5 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":447:12)
#loc12 = loc(unknown)
#loc49 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":81:42)
#loc57 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":90:25)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared1 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, colStride = 1>
#tmem1 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 1, colStride = 1>
#loc84 = loc("sm_scale"(#loc))
#loc85 = loc("M"(#loc))
#loc86 = loc("Z"(#loc))
#loc87 = loc("H"(#loc))
#loc88 = loc("desc_q"(#loc))
#loc89 = loc("desc_k"(#loc))
#loc90 = loc("desc_v"(#loc))
#loc91 = loc("desc_o"(#loc))
#loc94 = loc(callsite(#loc5 at #loc2))
#loc132 = loc("m_ij"(#loc49))
#loc138 = loc("l_ij"(#loc57))
#loc158 = loc(callsite(#loc4 at #loc94))
#loc193 = loc(callsite(#loc132 at #loc158))
#loc199 = loc(callsite(#loc138 at #loc158))
#loc214 = loc(callsite(#loc12 at #loc193))
#loc216 = loc(callsite(#loc12 at #loc199))
module attributes {"ttg.cluster-dim-x" = 1 : i32, "ttg.cluster-dim-y" = 1 : i32, "ttg.cluster-dim-z" = 1 : i32, ttg.max_reg_auto_ws = 152 : i32, ttg.min_reg_auto_ws = 24 : i32, "ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_fwd_persist(%sm_scale: f32 loc("sm_scale"(#loc)), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("M"(#loc)), %Z: i32 loc("Z"(#loc)), %H: i32 {tt.divisibility = 16 : i32} loc("H"(#loc)), %desc_q: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("desc_q"(#loc)), %desc_k: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("desc_k"(#loc)), %desc_v: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("desc_v"(#loc)), %desc_o: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("desc_o"(#loc))) attributes {noinline = false} {
    %0 = ttg.local_alloc {async_task_id = array<i32: 0>} : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc92)
    %1 = ttg.local_alloc {async_task_id = array<i32: 0>} : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc92)
    %acc = ttng.tmem_alloc : () -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
    %acc_0 = ttng.tmem_alloc : () -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
    %alpha, %alpha_1 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc188)
    %alpha_2, %alpha_3 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc188)
    %qk, %qk_4 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc189)
    %qk_5, %qk_6 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc189)
    %v = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc159)
    %k = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc160)
    %offsetkv_y, %offsetkv_y_7 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc220)
    %offsetkv_y_8, %offsetkv_y_9 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc220)
    %offsetkv_y_10, %offsetkv_y_11 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc220)
    %offsetkv_y_12, %offsetkv_y_13 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc220)
    %acc_14, %acc_15 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc187)
    %acc_16, %acc_17 = ttng.tmem_alloc : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc187)
    %q0 = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc162)
    %q0_18 = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc162)
    %false = arith.constant {async_task_id = array<i32: 1>} false loc(#loc12)
    %true = arith.constant {async_task_id = array<i32: 0, 1>} true loc(#loc12)
    %n_tile_num = arith.constant {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} 32 : i32 loc(#loc163)
    %c1_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} 1 : i32 loc(#loc12)
    %c8192_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} 8192 : i32 loc(#loc12)
    %c128_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} 128 : i32 loc(#loc12)
    %c128_i64 = arith.constant {async_task_id = array<i32: 2, 3>} 128 : i64 loc(#loc12)
    %c1_i64 = arith.constant {async_task_id = array<i32: 2, 3>} 1 : i64 loc(#loc12)
    %c0_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} 0 : i32 loc(#loc12)
    %c256_i32 = arith.constant {async_task_id = array<i32: 0, 2, 3>} 256 : i32 loc(#loc12)
    %cst = arith.constant {async_task_id = array<i32: 4, 5>} 1.44269502 : f32 loc(#loc12)
    %cst_19 = arith.constant {async_task_id = array<i32: 0>} dense<0.000000e+00> : tensor<128x128xf32, #blocked> loc(#loc12)
    %cst_20 = arith.constant {async_task_id = array<i32: 0, 4, 5>} dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc12)
    %cst_21 = arith.constant {async_task_id = array<i32: 0, 4, 5>} dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc12)
    %prog_id = tt.get_program_id x {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc102)
    %num_progs = tt.get_num_programs x {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc103)
    %total_tiles = arith.muli %Z, %n_tile_num {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc104)
    %total_tiles_22 = arith.muli %total_tiles, %H {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc105)
    %tiles_per_sm = arith.divsi %total_tiles_22, %num_progs {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc164)
    %2 = arith.remsi %total_tiles_22, %num_progs {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc20)
    %3 = arith.cmpi slt, %prog_id, %2 {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc21)
    %4 = scf.if %3 -> (i32) {
      %tiles_per_sm_35 = arith.addi %tiles_per_sm, %c1_i32 {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} : i32 loc(#loc165)
      scf.yield {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} %tiles_per_sm_35 : i32 loc(#loc165)
    } else {
      scf.yield {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} %tiles_per_sm : i32 loc(#loc12)
    } {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>} loc(#loc22)
    %desc_q_23 = arith.muli %Z, %H {async_task_id = array<i32: 2, 3>} : i32 loc(#loc108)
    %desc_q_24 = arith.muli %desc_q_23, %c8192_i32 {async_task_id = array<i32: 2, 3>} : i32 loc(#loc109)
    %desc_q_25 = tt.make_tensor_descriptor %desc_q, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 2>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc110)
    %desc_q_26 = tt.make_tensor_descriptor %desc_q, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 2>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc110)
    %desc_k_27 = tt.make_tensor_descriptor %desc_k, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 2>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc111)
    %desc_v_28 = tt.make_tensor_descriptor %desc_v, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 2>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc112)
    %desc_o_29 = tt.make_tensor_descriptor %desc_o, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 3>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc113)
    %desc_o_30 = tt.make_tensor_descriptor %desc_o, [%desc_q_24, %c128_i32], [%c128_i64, %c1_i64] {async_task_id = array<i32: 3>} : !tt.ptr<bf16>, !tt.tensordesc<tensor<128x128xbf16, #shared>> loc(#loc113)
    %offset_y = arith.muli %H, %c8192_i32 {async_task_id = array<i32: 2, 3>} : i32 loc(#loc166)
    %offs_m0 = tt.make_range {async_task_id = array<i32: 0>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1> loc(#loc167)
    %offs_m0_31 = tt.make_range {async_task_id = array<i32: 0>, end = 256 : i32, start = 128 : i32} : tensor<128xi32, #blocked1> loc(#loc167)
    %qk_scale = arith.mulf %sm_scale, %cst {async_task_id = array<i32: 4, 5>} : f32 loc(#loc168)
    %m_ij = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
    %m_ij_32 = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
    %qk_33 = tt.splat %qk_scale {async_task_id = array<i32: 5>} : f32 -> tensor<128x128xf32, #blocked> loc(#loc192)
    %qk_34 = tt.splat %qk_scale {async_task_id = array<i32: 4>} : f32 -> tensor<128x128xf32, #blocked> loc(#loc192)
    %tile_idx = scf.for %_ = %c0_i32 to %4 step %c1_i32 iter_args(%tile_idx_35 = %prog_id) -> (i32)  : i32 {
      %pid = arith.remsi %tile_idx_35, %n_tile_num {async_task_id = array<i32: 0, 2, 3>} : i32 loc(#loc120)
      %off_hz = arith.divsi %tile_idx_35, %n_tile_num {async_task_id = array<i32: 0, 2, 3>} : i32 loc(#loc121)
      %off_z = arith.divsi %off_hz, %H {async_task_id = array<i32: 2, 3>} : i32 loc(#loc169)
      %off_h = arith.remsi %off_hz, %H {async_task_id = array<i32: 2, 3>} : i32 loc(#loc170)
      %offset_y_36 = arith.muli %off_z, %offset_y {async_task_id = array<i32: 2, 3>} : i32 loc(#loc171)
      %offset_y_37 = arith.muli %off_h, %c8192_i32 {async_task_id = array<i32: 2, 3>} : i32 loc(#loc172)
      %offset_y_38 = arith.addi %offset_y_36, %offset_y_37 {async_task_id = array<i32: 2, 3>} : i32 loc(#loc173)
      %qo_offset_y = arith.muli %pid, %c256_i32 {async_task_id = array<i32: 0, 2, 3>} : i32 loc(#loc174)
      %qo_offset_y_39 = arith.addi %offset_y_38, %qo_offset_y {async_task_id = array<i32: 2, 3>} : i32 loc(#loc175)
      %5 = arith.addi %qo_offset_y_39, %c128_i32 {async_task_id = array<i32: 3>} : i32 loc(#loc129)
      %q0_40 = arith.addi %qo_offset_y_39, %c128_i32 {async_task_id = array<i32: 2>} : i32 loc(#loc162)
      %offs_m0_41 = tt.splat %qo_offset_y {async_task_id = array<i32: 0>} : i32 -> tensor<128xi32, #blocked1> loc(#loc176)
      %offs_m0_42 = tt.splat %qo_offset_y {async_task_id = array<i32: 0>} : i32 -> tensor<128xi32, #blocked1> loc(#loc176)
      %offs_m0_43 = arith.addi %offs_m0_41, %offs_m0 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc176)
      %offs_m0_44 = arith.addi %offs_m0_42, %offs_m0_31 {async_task_id = array<i32: 0>} : tensor<128xi32, #blocked1> loc(#loc176)
      %q0_45 = tt.descriptor_load %desc_q_25[%qo_offset_y_39, %c0_i32] {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc162)
      %q0_46 = tt.descriptor_load %desc_q_26[%q0_40, %c0_i32] {async_task_id = array<i32: 2>} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc162)
      ttg.local_store %q0_45, %q0_18 {async_task_id = array<i32: 2>} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc162)
      ttg.local_store %q0_46, %q0 {async_task_id = array<i32: 2>} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc162)
      %acc_47 = ttng.tmem_store %cst_19, %acc_16[%acc_17], %true {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
      %acc_48 = ttng.tmem_store %cst_19, %acc_14[%acc_15], %true {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
      %offsetkv_y_49:10 = scf.for %offsetkv_y_96 = %c0_i32 to %c8192_i32 step %c128_i32 iter_args(%offset_y_97 = %offset_y_38, %arg12 = %false, %arg13 = %cst_21, %arg14 = %cst_20, %qk_98 = %qk_6, %acc_99 = %acc_47, %arg17 = %cst_21, %arg18 = %cst_20, %qk_100 = %qk_4, %acc_101 = %acc_48) -> (i32, i1, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, !ttg.async.token, !ttg.async.token, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, !ttg.async.token, !ttg.async.token)  : i32 {
        %k_102 = tt.descriptor_load %desc_k_27[%offset_y_97, %c0_i32] {async_task_id = array<i32: 2>, loop.cluster = 1 : i32, loop.stage = 0 : i32} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc177)
        ttg.local_store %k_102, %k {async_task_id = array<i32: 2>, loop.cluster = 1 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc160)
        %k_103 = ttg.memdesc_trans %k {async_task_id = array<i32: 1>, loop.cluster = 1 : i32, loop.stage = 0 : i32, order = array<i32: 1, 0>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable> loc(#loc160)
        %v_104 = tt.descriptor_load %desc_v_28[%offset_y_97, %c0_i32] {async_task_id = array<i32: 2>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc159)
        ttg.local_store %v_104, %v {async_task_id = array<i32: 2>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc159)
        %qk_105 = ttng.tc_gen5_mma %q0_18, %k_103, %qk_5[%qk_98], %false, %true {async_task_id = array<i32: 1>, loop.cluster = 1 : i32, loop.stage = 0 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc189)
        %qk_106 = ttng.tc_gen5_mma %q0, %k_103, %qk[%qk_100], %false, %true {async_task_id = array<i32: 1>, loop.cluster = 3 : i32, loop.stage = 0 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared1, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc189)
        %qk_107, %qk_108 = ttng.tmem_load %qk_5[%qk_105] {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc189)
        %qk_109, %qk_110 = ttng.tmem_load %qk[%qk_106] {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc189)
        %m_ij_111 = "tt.reduce"(%qk_107) <{axis = 1 : i32}> ({
        ^bb0(%m_ij_182: f32 loc(callsite(#loc12 at #loc193)), %m_ij_183: f32 loc(callsite(#loc12 at #loc193))):
          %m_ij_184 = arith.maxnumf %m_ij_182, %m_ij_183 {async_task_id = array<i32: 5>} : f32 loc(#loc218)
          tt.reduce.return %m_ij_184 {async_task_id = array<i32: 5>} : f32 loc(#loc213)
        }) {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc213)
        %m_ij_112 = "tt.reduce"(%qk_109) <{axis = 1 : i32}> ({
        ^bb0(%m_ij_182: f32 loc(callsite(#loc12 at #loc193)), %m_ij_183: f32 loc(callsite(#loc12 at #loc193))):
          %m_ij_184 = arith.maxnumf %m_ij_182, %m_ij_183 {async_task_id = array<i32: 4>} : f32 loc(#loc218)
          tt.reduce.return %m_ij_184 {async_task_id = array<i32: 4>} : f32 loc(#loc213)
        }) {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc213)
        %m_ij_113 = arith.mulf %m_ij_111, %m_ij {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
        %m_ij_114 = arith.mulf %m_ij_112, %m_ij_32 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
        %m_ij_115 = arith.maxnumf %arg14, %m_ij_113 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc194)
        %m_ij_116 = arith.maxnumf %arg18, %m_ij_114 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc194)
        %qk_117 = arith.mulf %qk_107, %qk_33 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> loc(#loc192)
        %qk_118 = arith.mulf %qk_109, %qk_34 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> loc(#loc192)
        %qk_119 = tt.expand_dims %m_ij_115 {async_task_id = array<i32: 5>, axis = 1 : i32, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc195)
        %qk_120 = tt.expand_dims %m_ij_116 {async_task_id = array<i32: 4>, axis = 1 : i32, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc195)
        %qk_121 = tt.broadcast %qk_119 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc196)
        %qk_122 = tt.broadcast %qk_120 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc196)
        %qk_123 = arith.subf %qk_117, %qk_121 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> loc(#loc196)
        %qk_124 = arith.subf %qk_118, %qk_122 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> loc(#loc196)
        %p = math.exp2 %qk_123 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> loc(#loc197)
        %p_125 = math.exp2 %qk_124 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> loc(#loc197)
        %alpha_126 = arith.subf %arg14, %m_ij_115 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc198)
        %alpha_127 = arith.subf %arg18, %m_ij_116 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc198)
        %alpha_128 = math.exp2 %alpha_126 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc188)
        %alpha_129 = tt.expand_dims %alpha_128 {async_task_id = array<i32: 5>, axis = 1 : i32, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc188)
        %alpha_130 = ttg.convert_layout %alpha_129 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc188)
        %alpha_131 = arith.constant {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} true loc(#loc188)
        ttng.tmem_store %alpha_130, %alpha_2, %alpha_131 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc188)
        %alpha_132 = math.exp2 %alpha_127 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc188)
        %alpha_133 = tt.expand_dims %alpha_132 {async_task_id = array<i32: 4>, axis = 1 : i32, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc188)
        %alpha_134 = ttg.convert_layout %alpha_133 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc188)
        %alpha_135 = arith.constant {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} true loc(#loc188)
        ttng.tmem_store %alpha_134, %alpha, %alpha_135 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc188)
        %l_ij = "tt.reduce"(%p) <{axis = 1 : i32}> ({
        ^bb0(%l_ij_182: f32 loc(callsite(#loc12 at #loc199)), %l_ij_183: f32 loc(callsite(#loc12 at #loc199))):
          %l_ij_184 = arith.addf %l_ij_182, %l_ij_183 {async_task_id = array<i32: 5>} : f32 loc(#loc219)
          tt.reduce.return %l_ij_184 {async_task_id = array<i32: 5>} : f32 loc(#loc215)
        }) {async_task_id = array<i32: 5>, loop.cluster = 1 : i32, loop.stage = 1 : i32} : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc215)
        %l_ij_136 = "tt.reduce"(%p_125) <{axis = 1 : i32}> ({
        ^bb0(%l_ij_182: f32 loc(callsite(#loc12 at #loc199)), %l_ij_183: f32 loc(callsite(#loc12 at #loc199))):
          %l_ij_184 = arith.addf %l_ij_182, %l_ij_183 {async_task_id = array<i32: 4>} : f32 loc(#loc219)
          tt.reduce.return %l_ij_184 {async_task_id = array<i32: 4>} : f32 loc(#loc215)
        }) {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : (tensor<128x128xf32, #blocked>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc215)
        %acc_137, %acc_138 = ttng.tmem_load %acc_16[%acc_99] {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc187)
        %acc_139, %acc_140 = ttng.tmem_load %acc_14[%acc_101] {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc187)
        %14 = tt.reshape %acc_137 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> -> tensor<128x2x64xf32, #blocked4> loc(#loc200)
        %15 = tt.reshape %acc_139 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> -> tensor<128x2x64xf32, #blocked4> loc(#loc200)
        %16 = tt.trans %14 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc201)
        %17 = tt.trans %15 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc201)
        %outLHS, %outRHS = tt.split %16 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #blocked6> loc(#loc202)
        %outLHS_141, %outRHS_142 = tt.split %17 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #blocked6> loc(#loc202)
        %18 = ttg.convert_layout %outRHS {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc202)
        %19 = ttg.convert_layout %outRHS_142 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc202)
        %20 = ttg.convert_layout %outLHS {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc202)
        %21 = ttg.convert_layout %outLHS_141 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x64xf32, #blocked> loc(#loc202)
        %acc0_143, %acc0_144 = ttng.tmem_load %alpha_2[] {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc203)
        %acc0_145 = tt.reshape %acc0_143 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc203)
        %acc0_146 = ttg.convert_layout %acc0_145 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
        %acc0_147 = tt.expand_dims %acc0_146 {async_task_id = array<i32: 0>, axis = 1 : i32, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc203)
        %acc0_148, %acc0_149 = ttng.tmem_load %alpha[] {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc203)
        %acc0_150 = tt.reshape %acc0_148 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc203)
        %acc0_151 = ttg.convert_layout %acc0_150 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
        %acc0_152 = tt.expand_dims %acc0_151 {async_task_id = array<i32: 0>, axis = 1 : i32, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc203)
        %acc0_153 = tt.broadcast %acc0_147 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc204)
        %acc0_154 = tt.broadcast %acc0_152 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x1xf32, #blocked> -> tensor<128x64xf32, #blocked> loc(#loc204)
        %acc0_155 = arith.mulf %20, %acc0_153 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64xf32, #blocked> loc(#loc204)
        %acc0_156 = arith.mulf %21, %acc0_154 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked> loc(#loc204)
        %acc1 = arith.mulf %18, %acc0_153 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64xf32, #blocked> loc(#loc205)
        %acc1_157 = arith.mulf %19, %acc0_154 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked> loc(#loc205)
        %acc_158 = tt.join %acc0_155, %acc1 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x64xf32, #blocked> -> tensor<128x64x2xf32, #blocked7> loc(#loc206)
        %acc_159 = tt.join %acc0_156, %acc1_157 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked> -> tensor<128x64x2xf32, #blocked7> loc(#loc206)
        %acc_160 = tt.trans %acc_158 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32, order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #blocked7> -> tensor<128x2x64xf32, #blocked8> loc(#loc207)
        %acc_161 = tt.trans %acc_159 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32, order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #blocked7> -> tensor<128x2x64xf32, #blocked8> loc(#loc207)
        %acc_162 = ttg.convert_layout %acc_160 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x2x64xf32, #blocked8> -> tensor<128x2x64xf32, #linear1> loc(#loc207)
        %acc_163 = ttg.convert_layout %acc_161 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x2x64xf32, #blocked8> -> tensor<128x2x64xf32, #linear1> loc(#loc207)
        %acc_164 = tt.reshape %acc_162 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x2x64xf32, #linear1> -> tensor<128x128xf32, #linear2> loc(#loc208)
        %acc_165 = tt.reshape %acc_163 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x2x64xf32, #linear1> -> tensor<128x128xf32, #linear2> loc(#loc208)
        %p_166 = arith.truncf %p {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> to tensor<128x128xbf16, #blocked> loc(#loc209)
        %p_167 = arith.truncf %p_125 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> to tensor<128x128xbf16, #blocked> loc(#loc209)
        %acc_168 = ttg.convert_layout %p_166 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked> -> tensor<128x128xbf16, #blocked> loc(#loc187)
        %acc_169 = arith.constant {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} true loc(#loc187)
        ttng.tmem_store %acc_168, %acc_0, %acc_169 {async_task_id = array<i32: 5>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked> -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %acc_170 = ttg.convert_layout %p_167 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xbf16, #blocked> -> tensor<128x128xbf16, #blocked> loc(#loc187)
        %acc_171 = arith.constant {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} true loc(#loc187)
        ttng.tmem_store %acc_170, %acc, %acc_171 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xbf16, #blocked> -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %acc_172 = ttg.convert_layout %acc_164 {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #linear2> -> tensor<128x128xf32, #blocked> loc(#loc187)
        %acc_173 = ttg.convert_layout %acc_165 {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #linear2> -> tensor<128x128xf32, #blocked> loc(#loc187)
        %acc_174 = ttng.tmem_store %acc_172, %acc_16[%acc_138], %true {async_task_id = array<i32: 0>, loop.cluster = 5 : i32, loop.stage = 0 : i32} : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %acc_175 = ttng.tmem_store %acc_173, %acc_14[%acc_140], %true {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %acc_176 = ttng.tc_gen5_mma %acc_0, %v, %acc_16[%acc_174], %arg12, %true {async_task_id = array<i32: 1>, loop.cluster = 5 : i32, loop.stage = 0 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %acc_177 = ttng.tc_gen5_mma %acc, %v, %acc_14[%acc_175], %arg12, %true {async_task_id = array<i32: 1>, loop.cluster = 2 : i32, loop.stage = 1 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc187)
        %l_i0 = arith.mulf %arg13, %alpha_128 {async_task_id = array<i32: 5>, loop.cluster = 1 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc210)
        %l_i0_178 = arith.mulf %arg17, %alpha_132 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc210)
        %l_i0_179 = arith.addf %l_i0, %l_ij {async_task_id = array<i32: 5>, loop.cluster = 1 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc211)
        %l_i0_180 = arith.addf %l_i0_178, %l_ij_136 {async_task_id = array<i32: 4>, loop.cluster = 2 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc211)
        %offsetkv_y_181 = arith.addi %offset_y_97, %c128_i32 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : i32 loc(#loc178)
        scf.yield {async_task_id = array<i32: 0, 1, 2, 4, 5>} %offsetkv_y_181, %true, %l_i0_179, %m_ij_115, %qk_108, %acc_176, %l_i0_180, %m_ij_116, %qk_110, %acc_177 : i32, i1, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, !ttg.async.token, !ttg.async.token, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>>, !ttg.async.token, !ttg.async.token loc(#loc179)
      } {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>, tt.scheduled_max_stage = 1 : i32} loc(#loc220)
      %offsetkv_y_50 = tt.expand_dims %offsetkv_y_49#7 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc220)
      %offsetkv_y_51 = ttg.convert_layout %offsetkv_y_50 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc220)
      %offsetkv_y_52 = arith.constant {async_task_id = array<i32: 4>} true loc(#loc220)
      ttng.tmem_store %offsetkv_y_51, %offsetkv_y, %offsetkv_y_52 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc220)
      %offsetkv_y_53 = tt.expand_dims %offsetkv_y_49#6 {async_task_id = array<i32: 4>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc220)
      %offsetkv_y_54 = ttg.convert_layout %offsetkv_y_53 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc220)
      %offsetkv_y_55 = arith.constant {async_task_id = array<i32: 4>} true loc(#loc220)
      ttng.tmem_store %offsetkv_y_54, %offsetkv_y_8, %offsetkv_y_55 {async_task_id = array<i32: 4>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc220)
      %offsetkv_y_56 = tt.expand_dims %offsetkv_y_49#3 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc220)
      %offsetkv_y_57 = ttg.convert_layout %offsetkv_y_56 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc220)
      %offsetkv_y_58 = arith.constant {async_task_id = array<i32: 5>} true loc(#loc220)
      ttng.tmem_store %offsetkv_y_57, %offsetkv_y_10, %offsetkv_y_58 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc220)
      %offsetkv_y_59 = tt.expand_dims %offsetkv_y_49#2 {async_task_id = array<i32: 5>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc220)
      %offsetkv_y_60 = ttg.convert_layout %offsetkv_y_59 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked> -> tensor<128x1xf32, #blocked3> loc(#loc220)
      %offsetkv_y_61 = arith.constant {async_task_id = array<i32: 5>} true loc(#loc220)
      ttng.tmem_store %offsetkv_y_60, %offsetkv_y_12, %offsetkv_y_61 {async_task_id = array<i32: 5>} : tensor<128x1xf32, #blocked3> -> !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc220)
      %m_i0, %m_i0_62 = ttng.tmem_load %offsetkv_y_12[] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc180)
      %m_i0_63 = tt.reshape %m_i0 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc180)
      %m_i0_64 = ttg.convert_layout %m_i0_63 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc180)
      %m_i0_65 = math.log2 %m_i0_64 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc180)
      %m_i0_66, %m_i0_67 = ttng.tmem_load %offsetkv_y_10[] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc181)
      %m_i0_68 = tt.reshape %m_i0_66 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc181)
      %m_i0_69 = ttg.convert_layout %m_i0_68 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc181)
      %m_i0_70 = arith.addf %m_i0_69, %m_i0_65 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc181)
      %6 = ttg.convert_layout %m_i0_70 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128xf32, #blocked1> loc(#loc151)
      %m_ptrs0 = arith.muli %off_hz, %c8192_i32 {async_task_id = array<i32: 0>} : i32 loc(#loc182)
      %m_ptrs0_71 = tt.addptr %M, %m_ptrs0 {async_task_id = array<i32: 0>} : !tt.ptr<f32>, i32 loc(#loc183)
      %m_ptrs0_72 = tt.splat %m_ptrs0_71 {async_task_id = array<i32: 0>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc184)
      %m_ptrs0_73 = tt.addptr %m_ptrs0_72, %offs_m0_43 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc184)
      tt.store %m_ptrs0_73, %6 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc151)
      %acc0 = tt.expand_dims %m_i0_64 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc185)
      %acc0_74 = tt.broadcast %acc0 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc186)
      %acc_75, %acc_76 = ttng.tmem_load %acc_16[%offsetkv_y_49#5] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc187)
      %acc0_77 = arith.divf %acc_75, %acc0_74 {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> loc(#loc186)
      %7 = arith.truncf %acc0_77 {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> to tensor<128x128xbf16, #blocked> loc(#loc92)
      ttg.local_store %7, %1 {async_task_id = array<i32: 0>} : tensor<128x128xbf16, #blocked> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc92)
      %8 = ttg.local_load %1 {async_task_id = array<i32: 3>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> tensor<128x128xbf16, #blocked> loc(#loc92)
      %9 = ttg.convert_layout %8 {async_task_id = array<i32: 3>} : tensor<128x128xbf16, #blocked> -> tensor<128x128xbf16, #blocked2> loc(#loc92)
      tt.descriptor_store %desc_o_29[%qo_offset_y_39, %c0_i32], %9 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x128xbf16, #shared>>, tensor<128x128xbf16, #blocked2> loc(#loc129)
      %m_i0_78, %m_i0_79 = ttng.tmem_load %offsetkv_y_8[] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc180)
      %m_i0_80 = tt.reshape %m_i0_78 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc180)
      %m_i0_81 = ttg.convert_layout %m_i0_80 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc180)
      %m_i0_82 = math.log2 %m_i0_81 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc180)
      %m_i0_83, %m_i0_84 = ttng.tmem_load %offsetkv_y[] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x1xf32, #tmem1, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked3> loc(#loc181)
      %m_i0_85 = tt.reshape %m_i0_83 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked3> -> tensor<128xf32, #linear> loc(#loc181)
      %m_i0_86 = ttg.convert_layout %m_i0_85 {async_task_id = array<i32: 0>} : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc181)
      %m_i0_87 = arith.addf %m_i0_86, %m_i0_82 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc181)
      %10 = ttg.convert_layout %m_i0_87 {async_task_id = array<i32: 0>} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128xf32, #blocked1> loc(#loc151)
      %m_ptrs0_88 = tt.splat %m_ptrs0_71 {async_task_id = array<i32: 0>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc184)
      %m_ptrs0_89 = tt.addptr %m_ptrs0_88, %offs_m0_44 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1>, tensor<128xi32, #blocked1> loc(#loc184)
      tt.store %m_ptrs0_89, %10 {async_task_id = array<i32: 0>} : tensor<128x!tt.ptr<f32>, #blocked1> loc(#loc151)
      %acc0_90 = tt.expand_dims %m_i0_81 {async_task_id = array<i32: 0>, axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<128x1xf32, #blocked> loc(#loc185)
      %acc0_91 = tt.broadcast %acc0_90 {async_task_id = array<i32: 0>} : tensor<128x1xf32, #blocked> -> tensor<128x128xf32, #blocked> loc(#loc186)
      %acc_92, %acc_93 = ttng.tmem_load %acc_14[%offsetkv_y_49#9] {async_task_id = array<i32: 0>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked> loc(#loc187)
      %acc0_94 = arith.divf %acc_92, %acc0_91 {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> loc(#loc186)
      %11 = arith.truncf %acc0_94 {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked> to tensor<128x128xbf16, #blocked> loc(#loc92)
      ttg.local_store %11, %0 {async_task_id = array<i32: 0>} : tensor<128x128xbf16, #blocked> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc92)
      %12 = ttg.local_load %0 {async_task_id = array<i32: 3>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> tensor<128x128xbf16, #blocked> loc(#loc92)
      %13 = ttg.convert_layout %12 {async_task_id = array<i32: 3>} : tensor<128x128xbf16, #blocked> -> tensor<128x128xbf16, #blocked2> loc(#loc92)
      tt.descriptor_store %desc_o_30[%5, %c0_i32], %13 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x128xbf16, #shared>>, tensor<128x128xbf16, #blocked2> loc(#loc129)
      %tile_idx_95 = arith.addi %tile_idx_35, %num_progs {async_task_id = array<i32: 0, 2, 3>} : i32 loc(#loc157)
      scf.yield {async_task_id = array<i32: 0, 2, 3>} %tile_idx_95 : i32 loc(#loc82)
    } {async_task_id = array<i32: 0, 1, 2, 3, 4, 5>, tt.data_partition_factor = 2 : i32, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc119)
    tt.return loc(#loc83)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":484:43)
#loc3 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":119:23)
#loc6 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":88:25)
#loc7 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":74:19)
#loc8 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":176:24)
#loc9 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":175:12)
#loc10 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":170:8)
#loc11 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":417:21)
#loc13 = loc("/home/mren/MetaMain/triton/python/triton/language/standard.py":41:11)
#loc14 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":605:32)
#loc15 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":606:28)
#loc16 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":607:32)
#loc17 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":608:31)
#loc18 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":608:35)
#loc19 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":610:34)
#loc20 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":611:31)
#loc21 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":611:17)
#loc22 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":611:7)
#loc23 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":612:24)
#loc24 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":618:19)
#loc25 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":618:23)
#loc26 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":617:8)
#loc27 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":623:8)
#loc28 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":629:8)
#loc29 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":635:8)
#loc30 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":404:32)
#loc31 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":407:47)
#loc32 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":415:16)
#loc33 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":81:47)
#loc34 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":85:22)
#loc35 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":646:8)
#loc36 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":648:25)
#loc37 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":649:29)
#loc38 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":401:22)
#loc39 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":402:21)
#loc40 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":404:24)
#loc41 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":404:45)
#loc42 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":404:37)
#loc43 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":405:39)
#loc44 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":405:29)
#loc45 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":484:35)
#loc46 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":407:34)
#loc47 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":175:24)
#loc48 = loc("/home/mren/MetaMain/triton/python/triton/language/standard.py":189:40)
#loc50 = loc("/home/mren/MetaMain/triton/python/triton/language/standard.py":168:27)
#loc51 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":81:31)
#loc52 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":85:38)
#loc53 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":85:33)
#loc54 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":86:21)
#loc55 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":88:31)
#loc56 = loc("/home/mren/MetaMain/triton/python/triton/language/standard.py":301:36)
#loc58 = loc("/home/mren/MetaMain/triton/python/triton/language/standard.py":261:15)
#loc59 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":97:33)
#loc60 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":97:65)
#loc61 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":97:21)
#loc62 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":102:32)
#loc63 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":102:26)
#loc64 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":103:26)
#loc65 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":104:28)
#loc66 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":104:48)
#loc67 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":104:59)
#loc68 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":117:13)
#loc69 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":123:22)
#loc70 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":123:30)
#loc71 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":197:22)
#loc72 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":197:8)
#loc73 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":480:25)
#loc74 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":480:12)
#loc75 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":483:22)
#loc76 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":482:27)
#loc77 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":482:18)
#loc78 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":482:35)
#loc79 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":481:23)
#loc80 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":481:18)
#loc81 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":673:20)
#loc82 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":673:8)
#loc83 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":642:4)
#loc92 = loc(callsite(#loc1 at #loc2))
#loc93 = loc("acc"(#loc3))
#loc95 = loc("alpha"(#loc6))
#loc96 = loc("qk"(#loc7))
#loc97 = loc("v"(#loc8))
#loc98 = loc("k"(#loc9))
#loc99 = loc("acc0"(#loc10))
#loc100 = loc("q0"(#loc11))
#loc101 = loc("n_tile_num"(#loc14))
#loc102 = loc("prog_id"(#loc15))
#loc103 = loc("num_progs"(#loc16))
#loc104 = loc("total_tiles"(#loc17))
#loc105 = loc("total_tiles"(#loc18))
#loc106 = loc("tiles_per_sm"(#loc19))
#loc107 = loc("tiles_per_sm"(#loc23))
#loc108 = loc("desc_q"(#loc24))
#loc109 = loc("desc_q"(#loc25))
#loc110 = loc("desc_q"(#loc26))
#loc111 = loc("desc_k"(#loc27))
#loc112 = loc("desc_v"(#loc28))
#loc113 = loc("desc_o"(#loc29))
#loc114 = loc("offset_y"(#loc30))
#loc115 = loc("offs_m0"(#loc31))
#loc116 = loc("qk_scale"(#loc32))
#loc117 = loc("m_ij"(#loc33))
#loc118 = loc("qk"(#loc34))
#loc119 = loc("tile_idx"(#loc35))
#loc120 = loc("pid"(#loc36))
#loc121 = loc("off_hz"(#loc37))
#loc122 = loc("off_z"(#loc38))
#loc123 = loc("off_h"(#loc39))
#loc124 = loc("offset_y"(#loc40))
#loc125 = loc("offset_y"(#loc41))
#loc126 = loc("offset_y"(#loc42))
#loc127 = loc("qo_offset_y"(#loc43))
#loc128 = loc("qo_offset_y"(#loc44))
#loc129 = loc(callsite(#loc45 at #loc2))
#loc130 = loc("offs_m0"(#loc46))
#loc131 = loc("k"(#loc47))
#loc133 = loc("m_ij"(#loc51))
#loc134 = loc("qk"(#loc52))
#loc135 = loc("qk"(#loc53))
#loc136 = loc("p"(#loc54))
#loc137 = loc("alpha"(#loc55))
#loc139 = loc("acc0"(#loc62))
#loc140 = loc("acc0"(#loc63))
#loc141 = loc("acc1"(#loc64))
#loc142 = loc("acc"(#loc65))
#loc143 = loc("acc"(#loc66))
#loc144 = loc("acc"(#loc67))
#loc145 = loc("p"(#loc68))
#loc146 = loc("l_i0"(#loc69))
#loc147 = loc("l_i0"(#loc70))
#loc148 = loc("offsetkv_y"(#loc71))
#loc149 = loc("m_i0"(#loc73))
#loc150 = loc("m_i0"(#loc74))
#loc151 = loc(callsite(#loc75 at #loc2))
#loc152 = loc("m_ptrs0"(#loc76))
#loc153 = loc("m_ptrs0"(#loc77))
#loc154 = loc("m_ptrs0"(#loc78))
#loc155 = loc("acc0"(#loc79))
#loc156 = loc("acc0"(#loc80))
#loc157 = loc("tile_idx"(#loc81))
#loc159 = loc(callsite(#loc97 at #loc94))
#loc160 = loc(callsite(#loc98 at #loc94))
#loc161 = loc("l_i0"(#loc99))
#loc162 = loc(callsite(#loc100 at #loc2))
#loc163 = loc(callsite(#loc13 at #loc101))
#loc164 = loc("tiles_per_sm"(#loc106))
#loc165 = loc("tiles_per_sm"(#loc107))
#loc166 = loc(callsite(#loc114 at #loc2))
#loc167 = loc(callsite(#loc115 at #loc2))
#loc168 = loc(callsite(#loc116 at #loc2))
#loc169 = loc(callsite(#loc122 at #loc2))
#loc170 = loc(callsite(#loc123 at #loc2))
#loc171 = loc(callsite(#loc124 at #loc2))
#loc172 = loc(callsite(#loc125 at #loc2))
#loc173 = loc(callsite(#loc126 at #loc2))
#loc174 = loc(callsite(#loc127 at #loc2))
#loc175 = loc(callsite(#loc128 at #loc2))
#loc176 = loc(callsite(#loc130 at #loc2))
#loc177 = loc(callsite(#loc131 at #loc94))
#loc178 = loc(callsite(#loc148 at #loc94))
#loc179 = loc(callsite(#loc72 at #loc94))
#loc180 = loc(callsite(#loc149 at #loc2))
#loc181 = loc(callsite(#loc150 at #loc2))
#loc182 = loc(callsite(#loc152 at #loc2))
#loc183 = loc(callsite(#loc153 at #loc2))
#loc184 = loc(callsite(#loc154 at #loc2))
#loc185 = loc(callsite(#loc155 at #loc2))
#loc186 = loc(callsite(#loc156 at #loc2))
#loc187 = loc(callsite(#loc93 at #loc158))
#loc188 = loc(callsite(#loc95 at #loc158))
#loc189 = loc(callsite(#loc96 at #loc158))
#loc190 = loc("l_i0_1"(#loc161))
#loc191 = loc(callsite(#loc117 at #loc158))
#loc192 = loc(callsite(#loc118 at #loc158))
#loc194 = loc(callsite(#loc133 at #loc158))
#loc195 = loc(callsite(#loc134 at #loc158))
#loc196 = loc(callsite(#loc135 at #loc158))
#loc197 = loc(callsite(#loc136 at #loc158))
#loc198 = loc(callsite(#loc137 at #loc158))
#loc200 = loc(callsite(#loc59 at #loc158))
#loc201 = loc(callsite(#loc60 at #loc158))
#loc202 = loc(callsite(#loc61 at #loc158))
#loc203 = loc(callsite(#loc139 at #loc158))
#loc204 = loc(callsite(#loc140 at #loc158))
#loc205 = loc(callsite(#loc141 at #loc158))
#loc206 = loc(callsite(#loc142 at #loc158))
#loc207 = loc(callsite(#loc143 at #loc158))
#loc208 = loc(callsite(#loc144 at #loc158))
#loc209 = loc(callsite(#loc145 at #loc158))
#loc210 = loc(callsite(#loc146 at #loc158))
#loc211 = loc(callsite(#loc147 at #loc158))
#loc212 = loc("m_i0"(#loc190))
#loc213 = loc(callsite(#loc48 at #loc193))
#loc215 = loc(callsite(#loc56 at #loc199))
#loc217 = loc("offsetkv_y"(#loc212))
#loc218 = loc(callsite(#loc50 at #loc213))
#loc219 = loc(callsite(#loc58 at #loc215))
#loc220 = loc(callsite(#loc217 at #loc94))
