// RUN: triton-opt %s --nvgpu-test-ws-memory-planner=num-buffers=3 2>&1 | FileCheck %s

// Test case: FA BWD pattern using allocateTMemAllocs2 backtracking algorithm.
// This test verifies the TMEM buffer allocation for Flash Attention backward pass.
//
// The key buffers in allocation order:
//   [0] dk: liveness=[44-112) size=128x128 - accumulator, long-lived
//   [1] dv: liveness=[45-110) size=128x128 - accumulator, long-lived
//   [2] qkT: liveness=[56-61) size=128x128 - temp buffer, short-lived
//   [3] dpT: liveness=[72-77) size=128x128 - temp buffer, short-lived
//   [4] dq: liveness=[83-85) size=128x128 - output buffer, short-lived
//   [5] dv_interm: liveness=[67-69) size=128x64 - intermediate, short-lived
//
// The hasPotentialReuse matrix (non-zero entries):
//   hasPotentialReuse(qkT, dq) = 2  (exact size match, has dependency)
//   hasPotentialReuse(qkT, dv_interm) = 1  (partial size, has dependency)
//   hasPotentialReuse(dpT, dq) = 2  (exact size match, has dependency)
//   hasPotentialReuse(dq, qkT) = 2  (bidirectional)
//   hasPotentialReuse(dq, dpT) = 2  (bidirectional)
//   NOTE: hasPotentialReuse(dpT, dv_interm) = 0 (NO dependency!)
//
// With backtracking search, the algorithm finds:
//   - dq first tries qkT, but that blocks dv_interm â†’ backtrack
//   - dq then reuses dpT (buffer.id=6)
//   - dv_interm reuses qkT (buffer.id=5)

// CHECK-LABEL: tt.func public @_attn_bwd
// The key verification: dq reuses dpT (same buffer.id=6, with buffer.offset=0)
// CHECK: ttng.tmem_alloc {async_task_id = array<i32: 0, 3>, buffer.copy = 1 : i32, buffer.id = 6 : i32}
// CHECK: ttng.tmem_alloc {async_task_id = array<i32: 0, 2>, buffer.copy = 1 : i32, buffer.id = 6 : i32, buffer.offset = 0 : i32}

// -----// WarpSpec internal IR Dump After: doBufferAllocation
#blocked = #ttg.blocked<{sizePerThread = [1, 32], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 2, 64], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 64, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#blocked6 = #ttg.blocked<{sizePerThread = [1, 64], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked7 = #ttg.blocked<{sizePerThread = [1, 2, 32], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked8 = #ttg.blocked<{sizePerThread = [1, 32, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#blocked9 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#loc = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":812:0)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 16}>
#shared1 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = false, elementBitWidth = 32}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 64, transposed = false, elementBitWidth = 16}>
#shared3 = #ttg.nvmma_shared<{swizzlingByteWidth = 128, transposed = true, elementBitWidth = 16}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, colStride = 1>
#loc67 = loc("desc_q"(#loc))
#loc68 = loc("desc_k"(#loc))
#loc69 = loc("desc_v"(#loc))
#loc70 = loc("sm_scale"(#loc))
#loc71 = loc("desc_do"(#loc))
#loc72 = loc("desc_dq"(#loc))
#loc73 = loc("desc_dk"(#loc))
#loc74 = loc("desc_dv"(#loc))
#loc75 = loc("M"(#loc))
#loc76 = loc("D"(#loc))
#loc77 = loc("stride_z"(#loc))
#loc78 = loc("stride_h"(#loc))
#loc79 = loc("stride_tok"(#loc))
#loc80 = loc("BATCH"(#loc))
#loc81 = loc("H"(#loc))
#loc82 = loc("N_CTX"(#loc))
module attributes {"ttg.cluster-dim-x" = 1 : i32, "ttg.cluster-dim-y" = 1 : i32, "ttg.cluster-dim-z" = 1 : i32, ttg.max_reg_auto_ws = 152 : i32, ttg.min_reg_auto_ws = 24 : i32, "ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:100", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_bwd(%desc_q: !tt.tensordesc<tensor<128x128xbf16, #shared>> loc("desc_q"(#loc)), %desc_q_0: i32 loc("desc_q"(#loc)), %desc_q_1: i32 loc("desc_q"(#loc)), %desc_q_2: i64 loc("desc_q"(#loc)), %desc_q_3: i64 loc("desc_q"(#loc)), %desc_k: !tt.tensordesc<tensor<128x128xbf16, #shared>> loc("desc_k"(#loc)), %desc_k_4: i32 loc("desc_k"(#loc)), %desc_k_5: i32 loc("desc_k"(#loc)), %desc_k_6: i64 loc("desc_k"(#loc)), %desc_k_7: i64 loc("desc_k"(#loc)), %desc_v: !tt.tensordesc<tensor<128x128xbf16, #shared>> loc("desc_v"(#loc)), %desc_v_8: i32 loc("desc_v"(#loc)), %desc_v_9: i32 loc("desc_v"(#loc)), %desc_v_10: i64 loc("desc_v"(#loc)), %desc_v_11: i64 loc("desc_v"(#loc)), %sm_scale: f32 loc("sm_scale"(#loc)), %desc_do: !tt.tensordesc<tensor<128x128xbf16, #shared>> loc("desc_do"(#loc)), %desc_do_12: i32 loc("desc_do"(#loc)), %desc_do_13: i32 loc("desc_do"(#loc)), %desc_do_14: i64 loc("desc_do"(#loc)), %desc_do_15: i64 loc("desc_do"(#loc)), %desc_dq: !tt.tensordesc<tensor<128x32xf32, #shared1>> loc("desc_dq"(#loc)), %desc_dq_16: i32 loc("desc_dq"(#loc)), %desc_dq_17: i32 loc("desc_dq"(#loc)), %desc_dq_18: i64 loc("desc_dq"(#loc)), %desc_dq_19: i64 loc("desc_dq"(#loc)), %desc_dk: !tt.tensordesc<tensor<128x32xbf16, #shared2>> loc("desc_dk"(#loc)), %desc_dk_20: i32 loc("desc_dk"(#loc)), %desc_dk_21: i32 loc("desc_dk"(#loc)), %desc_dk_22: i64 loc("desc_dk"(#loc)), %desc_dk_23: i64 loc("desc_dk"(#loc)), %desc_dv: !tt.tensordesc<tensor<128x32xbf16, #shared2>> loc("desc_dv"(#loc)), %desc_dv_24: i32 loc("desc_dv"(#loc)), %desc_dv_25: i32 loc("desc_dv"(#loc)), %desc_dv_26: i64 loc("desc_dv"(#loc)), %desc_dv_27: i64 loc("desc_dv"(#loc)), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("M"(#loc)), %D: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("D"(#loc)), %stride_z: i32 {tt.divisibility = 16 : i32} loc("stride_z"(#loc)), %stride_h: i32 {tt.divisibility = 16 : i32} loc("stride_h"(#loc)), %stride_tok: i32 {tt.divisibility = 16 : i32} loc("stride_tok"(#loc)), %BATCH: i32 loc("BATCH"(#loc)), %H: i32 {tt.divisibility = 16 : i32} loc("H"(#loc)), %N_CTX: i32 {tt.divisibility = 16 : i32} loc("N_CTX"(#loc))) attributes {noinline = false} {
    %dsT = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc138)
    %dv = ttng.tmem_alloc : () -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc139)
    %do = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc140)
    %q = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc141)
    %false = arith.constant {async_task_id = array<i32: 0>} false loc(#loc6)
    %true = arith.constant {async_task_id = array<i32: 0>} true loc(#loc6)
    %c128_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3>} 128 : i32 loc(#loc6)
    %c0_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3>} 0 : i32 loc(#loc6)
    %c1_i32 = arith.constant {async_task_id = array<i32: 0, 1, 2, 3>} 1 : i32 loc(#loc87)
    %cst = arith.constant {async_task_id = array<i32: 2>} dense<0.693147182> : tensor<128x32xf32, #blocked> loc(#loc6)
    %cst_28 = arith.constant {async_task_id = array<i32: 0>} dense<0.000000e+00> : tensor<128x128xf32, #blocked1> loc(#loc6)
    %bhid = tt.get_program_id z {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc88)
    %off_chz = arith.muli %bhid, %N_CTX {async_task_id = array<i32: 3>} : i32 loc(#loc89)
    %off_chz_29 = arith.extsi %off_chz {async_task_id = array<i32: 3>} : i32 to i64 loc(#loc90)
    %off_bh = arith.remsi %bhid, %H {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc91)
    %off_bh_30 = arith.muli %stride_h, %off_bh {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc92)
    %off_bh_31 = arith.divsi %bhid, %H {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc93)
    %off_bh_32 = arith.muli %stride_z, %off_bh_31 {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc94)
    %off_bh_33 = arith.addi %off_bh_30, %off_bh_32 {async_task_id = array<i32: 1, 2, 3>} : i32 loc(#loc95)
    %off_bh_34 = arith.extsi %off_bh_33 {async_task_id = array<i32: 1, 2, 3>} : i32 to i64 loc(#loc96)
    %off_bh_35 = arith.extsi %stride_tok {async_task_id = array<i32: 1, 2, 3>} : i32 to i64 loc(#loc97)
    %off_bh_36 = arith.divsi %off_bh_34, %off_bh_35 {async_task_id = array<i32: 1, 2, 3>} : i64 loc(#loc97)
    %pid = tt.get_program_id x {async_task_id = array<i32: 1, 3>} : i32 loc(#loc98)
    %M_37 = tt.addptr %M, %off_chz_29 {async_task_id = array<i32: 3>} : !tt.ptr<f32>, i64 loc(#loc99)
    %D_38 = tt.addptr %D, %off_chz_29 {async_task_id = array<i32: 3>} : !tt.ptr<f32>, i64 loc(#loc100)
    %start_n = arith.muli %pid, %c128_i32 {async_task_id = array<i32: 1, 3>} : i32 loc(#loc101)
    %k = arith.extsi %start_n {async_task_id = array<i32: 1, 3>} : i32 to i64 loc(#loc102)
    %k_39 = arith.addi %off_bh_36, %k {async_task_id = array<i32: 1, 3>} : i64 loc(#loc102)
    %k_40 = arith.trunci %k_39 {async_task_id = array<i32: 1, 3>} : i64 to i32 loc(#loc103)
    %k_41 = tt.descriptor_load %desc_k[%k_40, %c0_i32] {async_task_id = array<i32: 1>} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc104)
    %k_42 = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc104)
    ttg.local_store %k_41, %k_42 {async_task_id = array<i32: 1>} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc104)
    %v = tt.descriptor_load %desc_v[%k_40, %c0_i32] {async_task_id = array<i32: 1>} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc105)
    %v_43 = ttg.local_alloc : () -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc105)
    ttg.local_store %v, %v_43 {async_task_id = array<i32: 1>} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc105)
    %num_steps = arith.divsi %N_CTX, %c128_i32 {async_task_id = array<i32: 0, 1, 2, 3>} : i32 loc(#loc106)
    %offs_m = tt.make_range {async_task_id = array<i32: 3>, end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3> loc(#loc142)
    %m = tt.splat %M_37 {async_task_id = array<i32: 3>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked3> loc(#loc143)
    %Di = tt.splat %D_38 {async_task_id = array<i32: 3>} : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked3> loc(#loc144)
    %qkT, %qkT_44 = ttng.tmem_alloc {async_task_id = array<i32: 0, 3>} : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc145)
    %dv_45, %dv_46 = ttng.tmem_alloc {async_task_id = array<i32: 0, 3>} : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc139)
    %dpT, %dpT_47 = ttng.tmem_alloc {async_task_id = array<i32: 0, 3>} : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc146)
    %dk, %dk_48 = ttng.tmem_alloc {async_task_id = array<i32: 0, 3>} : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc147)
    %dq, %dq_49 = ttng.tmem_alloc {async_task_id = array<i32: 0, 2>} : () -> (!ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.async.token) loc(#loc148)
    %dk_50 = ttng.tmem_store %cst_28, %dk[%dk_48], %true {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked1> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc147)
    %dv_51 = ttng.tmem_store %cst_28, %dv_45[%dv_46], %true {async_task_id = array<i32: 0>} : tensor<128x128xf32, #blocked1> -> !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc139)
    %curr_m:7 = scf.for %curr_m_82 = %c0_i32 to %num_steps step %c1_i32 iter_args(%arg45 = %c0_i32, %arg46 = %false, %qkT_83 = %qkT_44, %dv_84 = %dv_51, %dpT_85 = %dpT_47, %dk_86 = %dk_50, %dq_87 = %dq_49) -> (i32, i1, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token)  : i32 {
      %q_88 = arith.extsi %arg45 {async_task_id = array<i32: 1, 2>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 to i64 loc(#loc150)
      %q_89 = arith.addi %off_bh_36, %q_88 {async_task_id = array<i32: 1, 2>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : i64 loc(#loc150)
      %q_90 = arith.trunci %q_89 {async_task_id = array<i32: 1, 2>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : i64 to i32 loc(#loc151)
      %q_91 = tt.descriptor_load %desc_q[%q_90, %c0_i32] {async_task_id = array<i32: 1>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc141)
      ttg.local_store %q_91, %q {async_task_id = array<i32: 1>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc141)
      %qT = ttg.memdesc_trans %q {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 0 : i32, order = array<i32: 1, 0>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable> loc(#loc152)
      %offs_m_92 = tt.splat %arg45 {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : i32 -> tensor<128xi32, #blocked3> loc(#loc153)
      %offs_m_93 = arith.addi %offs_m_92, %offs_m {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128xi32, #blocked3> loc(#loc153)
      %m_94 = tt.addptr %m, %offs_m_93 {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x!tt.ptr<f32>, #blocked3>, tensor<128xi32, #blocked3> loc(#loc143)
      %m_95 = tt.load %m_94 {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x!tt.ptr<f32>, #blocked3> loc(#loc154)
      %qkT_96 = ttng.tc_gen5_mma %k_42, %qT, %qkT[%qkT_83], %false, %true {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 0 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc145)
      %pT = ttg.convert_layout %m_95 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128xf32, #blocked3> -> tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc155)
      %pT_97 = tt.expand_dims %pT {async_task_id = array<i32: 3>, axis = 0 : i32, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xf32, #blocked1> loc(#loc156)
      %pT_98 = tt.broadcast %pT_97 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<1x128xf32, #blocked1> -> tensor<128x128xf32, #blocked1> loc(#loc155)
      %qkT_99, %qkT_100 = ttng.tmem_load %qkT[%qkT_96] {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked1> loc(#loc145)
      %pT_101 = arith.subf %qkT_99, %pT_98 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> loc(#loc155)
      %pT_102 = math.exp2 %pT_101 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> loc(#loc157)
      %do_103 = tt.descriptor_load %desc_do[%q_90, %c0_i32] {async_task_id = array<i32: 1>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : !tt.tensordesc<tensor<128x128xbf16, #shared>> -> tensor<128x128xbf16, #blocked2> loc(#loc140)
      ttg.local_store %do_103, %do {async_task_id = array<i32: 1>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x128xbf16, #blocked2> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc140)
      %ppT = arith.truncf %pT_102 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> to tensor<128x128xbf16, #blocked1> loc(#loc158)
      %dv_104 = arith.constant {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} true loc(#loc139)
      ttng.tmem_store %ppT, %dv, %dv_104 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xbf16, #blocked1> -> !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable> loc(#loc139)
      %dv_105 = ttng.tc_gen5_mma %dv, %do, %dv_45[%dv_84], %arg46, %true {async_task_id = array<i32: 0>, loop.cluster = 0 : i32, loop.stage = 1 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc139)
      %Di_106 = tt.addptr %Di, %offs_m_93 {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x!tt.ptr<f32>, #blocked3>, tensor<128xi32, #blocked3> loc(#loc144)
      %Di_107 = tt.load %Di_106 {async_task_id = array<i32: 3>, loop.cluster = 2 : i32, loop.stage = 0 : i32} : tensor<128x!tt.ptr<f32>, #blocked3> loc(#loc159)
      %dpT_108 = ttg.memdesc_trans %do {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 0 : i32, order = array<i32: 1, 0>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable> loc(#loc160)
      %dpT_109 = ttng.tc_gen5_mma %v_43, %dpT_108, %dpT[%dpT_85], %false, %true {async_task_id = array<i32: 0>, loop.cluster = 2 : i32, loop.stage = 0 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc146)
      %dsT_110 = ttg.convert_layout %Di_107 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128xf32, #blocked3> -> tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc161)
      %dsT_111 = tt.expand_dims %dsT_110 {async_task_id = array<i32: 3>, axis = 0 : i32, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x128xf32, #blocked1> loc(#loc162)
      %dsT_112 = tt.broadcast %dsT_111 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<1x128xf32, #blocked1> -> tensor<128x128xf32, #blocked1> loc(#loc161)
      %dpT_113, %dpT_114 = ttng.tmem_load %dpT[%dpT_109] {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked1> loc(#loc146)
      %dsT_115 = arith.subf %dpT_113, %dsT_112 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> loc(#loc161)
      %dsT_116 = arith.mulf %pT_102, %dsT_115 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> loc(#loc163)
      %dsT_117 = arith.truncf %dsT_116 {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> to tensor<128x128xbf16, #blocked1> loc(#loc138)
      ttg.local_store %dsT_117, %dsT {async_task_id = array<i32: 3>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xbf16, #blocked1> -> !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> loc(#loc138)
      %dk_118 = ttng.tc_gen5_mma %dsT, %q, %dk[%dk_86], %arg46, %true {async_task_id = array<i32: 0>, loop.cluster = 0 : i32, loop.stage = 1 : i32, tt.self_latency = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc147)
      %dq_119 = ttg.memdesc_trans %dsT {async_task_id = array<i32: 0>, loop.cluster = 0 : i32, loop.stage = 1 : i32, order = array<i32: 1, 0>} : !ttg.memdesc<128x128xbf16, #shared, #smem, mutable> -> !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable> loc(#loc164)
      %dq_120 = ttng.tc_gen5_mma %dq_119, %k_42, %dq[%dq_87], %false, %true {async_task_id = array<i32: 0>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xbf16, #shared3, #smem, mutable>, !ttg.memdesc<128x128xbf16, #shared, #smem, mutable>, !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc148)
      %dq_121, %dq_122 = ttng.tmem_load %dq[%dq_120] {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked1> loc(#loc148)
      %dqs = tt.reshape %dq_121 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x128xf32, #blocked1> -> tensor<128x2x64xf32, #blocked4> loc(#loc179)
      %dqs_123 = tt.trans %dqs {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc180)
      %dqs_124, %dqs_125 = tt.split %dqs_123 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #blocked6> loc(#loc181)
      %dqs_126 = tt.reshape %dqs_124 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc197)
      %dqs_127 = tt.trans %dqs_126 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc198)
      %dqs_128, %dqs_129 = tt.split %dqs_127 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc199)
      %dqs_130 = tt.reshape %dqs_125 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc200)
      %dqs_131 = tt.trans %dqs_130 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc201)
      %dqs_132, %dqs_133 = tt.split %dqs_131 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc202)
      %dqN = arith.mulf %dqs_128, %cst {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> loc(#loc166)
      %dqN_134 = ttg.convert_layout %dqN {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> -> tensor<128x32xf32, #blocked9> loc(#loc166)
      tt.descriptor_reduce add, %desc_dq[%q_90, %c0_i32], %dqN_134 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !tt.tensordesc<tensor<128x32xf32, #shared1>>, tensor<128x32xf32, #blocked9> loc(#loc132)
      %dqN_135 = arith.mulf %dqs_129, %cst {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> loc(#loc166)
      %dqN_136 = ttg.convert_layout %dqN_135 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> -> tensor<128x32xf32, #blocked9> loc(#loc166)
      tt.descriptor_reduce add, %desc_dq[%q_90, %c0_i32], %dqN_136 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !tt.tensordesc<tensor<128x32xf32, #shared1>>, tensor<128x32xf32, #blocked9> loc(#loc132)
      %dqN_137 = arith.mulf %dqs_132, %cst {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> loc(#loc166)
      %dqN_138 = ttg.convert_layout %dqN_137 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> -> tensor<128x32xf32, #blocked9> loc(#loc166)
      tt.descriptor_reduce add, %desc_dq[%q_90, %c0_i32], %dqN_138 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !tt.tensordesc<tensor<128x32xf32, #shared1>>, tensor<128x32xf32, #blocked9> loc(#loc132)
      %dqN_139 = arith.mulf %dqs_133, %cst {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> loc(#loc166)
      %dqN_140 = ttg.convert_layout %dqN_139 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : tensor<128x32xf32, #blocked> -> tensor<128x32xf32, #blocked9> loc(#loc166)
      tt.descriptor_reduce add, %desc_dq[%q_90, %c0_i32], %dqN_140 {async_task_id = array<i32: 2>, loop.cluster = 0 : i32, loop.stage = 1 : i32} : !tt.tensordesc<tensor<128x32xf32, #shared1>>, tensor<128x32xf32, #blocked9> loc(#loc132)
      %curr_m_141 = arith.addi %arg45, %c128_i32 {async_task_id = array<i32: 1, 2, 3>, loop.cluster = 1 : i32, loop.stage = 1 : i32} : i32 loc(#loc167)
      scf.yield {async_task_id = array<i32: 0, 1, 2, 3>} %curr_m_141, %true, %qkT_100, %dv_105, %dpT_114, %dk_118, %dq_122 : i32, i1, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token, !ttg.async.token loc(#loc134)
    } {async_task_id = array<i32: 0, 1, 2, 3>, tt.merge_epilogue = true, tt.scheduled_max_stage = 1 : i32, tt.warp_specialize, ttg.partition.stages = [0 : i32, 1 : i32, 0 : i32, 0 : i32, 0 : i32], ttg.warp_specialize.tag = 0 : i32} loc(#loc203)
    %dv_52, %dv_53 = ttng.tmem_load %dv_45[%curr_m#3] {async_task_id = array<i32: 3>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked1> loc(#loc139)
    %dvs = tt.reshape %dv_52 {async_task_id = array<i32: 3>} : tensor<128x128xf32, #blocked1> -> tensor<128x2x64xf32, #blocked4> loc(#loc168)
    %dk_54, %dk_55 = ttng.tmem_load %dk[%curr_m#5] {async_task_id = array<i32: 3>} : !ttg.memdesc<128x128xf32, #tmem, #ttng.tensor_memory, mutable> -> tensor<128x128xf32, #blocked1> loc(#loc147)
    %dks = tt.reshape %dk_54 {async_task_id = array<i32: 3>} : tensor<128x128xf32, #blocked1> -> tensor<128x2x64xf32, #blocked4> loc(#loc169)
    %dvs_56 = tt.trans %dvs {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc170)
    %dvs_57, %dvs_58 = tt.split %dvs_56 {async_task_id = array<i32: 3>} : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #blocked6> loc(#loc171)
    %dvs_59 = tt.reshape %dvs_58 {async_task_id = array<i32: 3>} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc184)
    %dvs_60 = tt.reshape %dvs_57 {async_task_id = array<i32: 3>} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc185)
    %dvs_61 = tt.trans %dvs_60 {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc186)
    %dvs_62, %dvs_63 = tt.split %dvs_61 {async_task_id = array<i32: 3>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc187)
    %0 = arith.truncf %dvs_63 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc61)
    %1 = arith.truncf %dvs_62 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc61)
    %dvs_64 = tt.trans %dvs_59 {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc188)
    %dvs_65, %dvs_66 = tt.split %dvs_64 {async_task_id = array<i32: 3>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc189)
    %2 = arith.truncf %dvs_66 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc61)
    %3 = arith.truncf %dvs_65 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc61)
    %4 = ttg.convert_layout %1 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc61)
    tt.descriptor_store %desc_dv[%k_40, %c0_i32], %4 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc62)
    %5 = ttg.convert_layout %0 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc61)
    tt.descriptor_store %desc_dv[%k_40, %c0_i32], %5 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc62)
    %6 = ttg.convert_layout %3 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc61)
    tt.descriptor_store %desc_dv[%k_40, %c0_i32], %6 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc62)
    %7 = ttg.convert_layout %2 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc61)
    tt.descriptor_store %desc_dv[%k_40, %c0_i32], %7 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc62)
    %dks_67 = tt.trans %dks {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc174)
    %dks_68, %dks_69 = tt.split %dks_67 {async_task_id = array<i32: 3>} : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #blocked6> loc(#loc175)
    %dks_70 = tt.reshape %dks_69 {async_task_id = array<i32: 3>} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc190)
    %dks_71 = tt.reshape %dks_68 {async_task_id = array<i32: 3>} : tensor<128x64xf32, #blocked6> -> tensor<128x2x32xf32, #blocked7> loc(#loc191)
    %dks_72 = tt.trans %dks_71 {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc192)
    %dks_73, %dks_74 = tt.split %dks_72 {async_task_id = array<i32: 3>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc193)
    %dks_75 = tt.trans %dks_70 {async_task_id = array<i32: 3>, order = array<i32: 0, 2, 1>} : tensor<128x2x32xf32, #blocked7> -> tensor<128x32x2xf32, #blocked8> loc(#loc194)
    %dks_76, %dks_77 = tt.split %dks_75 {async_task_id = array<i32: 3>} : tensor<128x32x2xf32, #blocked8> -> tensor<128x32xf32, #blocked> loc(#loc195)
    %dkN = tt.splat %sm_scale {async_task_id = array<i32: 3>} : f32 -> tensor<128x32xf32, #blocked> loc(#loc137)
    %dkN_78 = arith.mulf %dks_77, %dkN {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> loc(#loc137)
    %dkN_79 = arith.mulf %dks_76, %dkN {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> loc(#loc137)
    %dkN_80 = arith.mulf %dks_74, %dkN {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> loc(#loc137)
    %dkN_81 = arith.mulf %dks_73, %dkN {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> loc(#loc137)
    %8 = arith.truncf %dkN_81 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc64)
    %9 = ttg.convert_layout %8 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc64)
    tt.descriptor_store %desc_dk[%k_40, %c0_i32], %9 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc65)
    %10 = arith.truncf %dkN_80 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc64)
    %11 = ttg.convert_layout %10 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc64)
    tt.descriptor_store %desc_dk[%k_40, %c0_i32], %11 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc65)
    %12 = arith.truncf %dkN_79 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc64)
    %13 = ttg.convert_layout %12 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc64)
    tt.descriptor_store %desc_dk[%k_40, %c0_i32], %13 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc65)
    %14 = arith.truncf %dkN_78 {async_task_id = array<i32: 3>} : tensor<128x32xf32, #blocked> to tensor<128x32xbf16, #blocked> loc(#loc64)
    %15 = ttg.convert_layout %14 {async_task_id = array<i32: 3>} : tensor<128x32xbf16, #blocked> -> tensor<128x32xbf16, #blocked9> loc(#loc64)
    tt.descriptor_store %desc_dk[%k_40, %c0_i32], %15 {async_task_id = array<i32: 3>} : !tt.tensordesc<tensor<128x32xbf16, #shared2>>, tensor<128x32xbf16, #blocked9> loc(#loc65)
    tt.return loc(#loc66)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":764:21)
#loc2 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":929:8)
#loc3 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":758:26)
#loc4 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":754:26)
#loc5 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":743:24)
#loc6 = loc(unknown)
#loc7 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":742:75)
#loc8 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":841:25)
#loc9 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":842:22)
#loc10 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":842:32)
#loc11 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:28)
#loc12 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:21)
#loc13 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:53)
#loc14 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:45)
#loc15 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:33)
#loc16 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":844:60)
#loc17 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":845:9)
#loc18 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":846:24)
#loc19 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":849:9)
#loc20 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":850:9)
#loc21 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":900:20)
#loc22 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":904:31)
#loc23 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":904:43)
#loc24 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":904:20)
#loc25 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":905:20)
#loc26 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":907:37)
#loc27 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":746:39)
#loc28 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":747:24)
#loc29 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":760:25)
#loc30 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":748:24)
#loc31 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":762:24)
#loc32 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":765:26)
#loc33 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":767:35)
#loc34 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":743:35)
#loc35 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":743:46)
#loc36 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":744:22)
#loc37 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":746:26)
#loc38 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":747:20)
#loc39 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":749:32)
#loc40 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":749:34)
#loc41 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":749:26)
#loc42 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":757:21)
#loc43 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":760:21)
#loc44 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":762:33)
#loc45 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":763:26)
#loc46 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":763:29)
#loc47 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":763:20)
#loc48 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":767:29)
#loc49 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":51:27)
#loc50 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":768:27)
#loc51 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":51:75)
#loc52 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":51:17)
#loc53 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":52:28)
#loc54 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":52:62)
#loc55 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":770:34)
#loc56 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":771:68)
#loc57 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":773:18)
#loc58 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":773:8)
#loc59 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":936:23)
#loc60 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":945:23)
#loc61 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":941:19)
#loc62 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":941:12)
#loc63 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":947:30)
#loc64 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":950:19)
#loc65 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":950:12)
#loc66 = loc("/home/mren/OpenSource/tritonbench/tritonbench/kernels/blackwell_triton_fused_attention.py":946:4)
#loc83 = loc("dsT"(#loc1))
#loc84 = loc("dv"(#loc3))
#loc85 = loc("do"(#loc4))
#loc86 = loc("q"(#loc5))
#loc87 = loc(callsite(#loc7 at #loc2))
#loc88 = loc("bhid"(#loc8))
#loc89 = loc("off_chz"(#loc9))
#loc90 = loc("off_chz"(#loc10))
#loc91 = loc("off_bh"(#loc11))
#loc92 = loc("off_bh"(#loc12))
#loc93 = loc("off_bh"(#loc13))
#loc94 = loc("off_bh"(#loc14))
#loc95 = loc("off_bh"(#loc15))
#loc96 = loc("off_bh"(#loc16))
#loc97 = loc("off_bh"(#loc17))
#loc98 = loc("pid"(#loc18))
#loc99 = loc("M"(#loc19))
#loc100 = loc("D"(#loc20))
#loc101 = loc("start_n"(#loc21))
#loc102 = loc("k"(#loc22))
#loc103 = loc("k"(#loc23))
#loc104 = loc("k"(#loc24))
#loc105 = loc("v"(#loc25))
#loc106 = loc("num_steps"(#loc26))
#loc107 = loc("offs_m"(#loc27))
#loc108 = loc("m"(#loc28))
#loc109 = loc("Di"(#loc29))
#loc110 = loc("qkT"(#loc30))
#loc111 = loc("dpT"(#loc31))
#loc112 = loc("dk"(#loc32))
#loc113 = loc("dq"(#loc33))
#loc114 = loc("dk"(#loc7))
#loc115 = loc("q"(#loc34))
#loc116 = loc("q"(#loc35))
#loc117 = loc("qT"(#loc36))
#loc118 = loc("offs_m"(#loc37))
#loc119 = loc("m"(#loc38))
#loc120 = loc("pT"(#loc39))
#loc121 = loc("pT"(#loc40))
#loc122 = loc("pT"(#loc41))
#loc123 = loc("ppT"(#loc42))
#loc124 = loc("Di"(#loc43))
#loc125 = loc("dpT"(#loc44))
#loc126 = loc("dsT"(#loc45))
#loc127 = loc("dsT"(#loc46))
#loc128 = loc("dsT"(#loc47))
#loc129 = loc("dq"(#loc48))
#loc130 = loc("dqs"(#loc50))
#loc131 = loc("dqN"(#loc55))
#loc132 = loc(callsite(#loc56 at #loc2))
#loc133 = loc("curr_m"(#loc57))
#loc134 = loc(callsite(#loc58 at #loc2))
#loc135 = loc("dvs"(#loc59))
#loc136 = loc("dks"(#loc60))
#loc137 = loc("dkN"(#loc63))
#loc138 = loc(callsite(#loc83 at #loc2))
#loc139 = loc(callsite(#loc84 at #loc2))
#loc140 = loc(callsite(#loc85 at #loc2))
#loc141 = loc(callsite(#loc86 at #loc2))
#loc142 = loc(callsite(#loc107 at #loc2))
#loc143 = loc(callsite(#loc108 at #loc2))
#loc144 = loc(callsite(#loc109 at #loc2))
#loc145 = loc(callsite(#loc110 at #loc2))
#loc146 = loc(callsite(#loc111 at #loc2))
#loc147 = loc(callsite(#loc112 at #loc2))
#loc148 = loc(callsite(#loc113 at #loc2))
#loc149 = loc("dv"(#loc114))
#loc150 = loc(callsite(#loc115 at #loc2))
#loc151 = loc(callsite(#loc116 at #loc2))
#loc152 = loc(callsite(#loc117 at #loc2))
#loc153 = loc(callsite(#loc118 at #loc2))
#loc154 = loc(callsite(#loc119 at #loc2))
#loc155 = loc(callsite(#loc120 at #loc2))
#loc156 = loc(callsite(#loc121 at #loc2))
#loc157 = loc(callsite(#loc122 at #loc2))
#loc158 = loc(callsite(#loc123 at #loc2))
#loc159 = loc(callsite(#loc124 at #loc2))
#loc160 = loc(callsite(#loc125 at #loc2))
#loc161 = loc(callsite(#loc126 at #loc2))
#loc162 = loc(callsite(#loc127 at #loc2))
#loc163 = loc(callsite(#loc128 at #loc2))
#loc164 = loc(callsite(#loc129 at #loc2))
#loc165 = loc(callsite(#loc130 at #loc2))
#loc166 = loc(callsite(#loc131 at #loc2))
#loc167 = loc(callsite(#loc133 at #loc2))
#loc168 = loc(callsite(#loc49 at #loc135))
#loc169 = loc(callsite(#loc49 at #loc136))
#loc170 = loc(callsite(#loc51 at #loc135))
#loc171 = loc(callsite(#loc52 at #loc135))
#loc172 = loc(callsite(#loc54 at #loc135))
#loc173 = loc(callsite(#loc53 at #loc135))
#loc174 = loc(callsite(#loc51 at #loc136))
#loc175 = loc(callsite(#loc52 at #loc136))
#loc176 = loc(callsite(#loc54 at #loc136))
#loc177 = loc(callsite(#loc53 at #loc136))
#loc178 = loc("offs_m"(#loc149))
#loc179 = loc(callsite(#loc49 at #loc165))
#loc180 = loc(callsite(#loc51 at #loc165))
#loc181 = loc(callsite(#loc52 at #loc165))
#loc182 = loc(callsite(#loc53 at #loc165))
#loc183 = loc(callsite(#loc54 at #loc165))
#loc184 = loc(callsite(#loc49 at #loc172))
#loc185 = loc(callsite(#loc49 at #loc173))
#loc186 = loc(callsite(#loc51 at #loc173))
#loc187 = loc(callsite(#loc52 at #loc173))
#loc188 = loc(callsite(#loc51 at #loc172))
#loc189 = loc(callsite(#loc52 at #loc172))
#loc190 = loc(callsite(#loc49 at #loc176))
#loc191 = loc(callsite(#loc49 at #loc177))
#loc192 = loc(callsite(#loc51 at #loc177))
#loc193 = loc(callsite(#loc52 at #loc177))
#loc194 = loc(callsite(#loc51 at #loc176))
#loc195 = loc(callsite(#loc52 at #loc176))
#loc196 = loc("curr_m"(#loc178))
#loc197 = loc(callsite(#loc49 at #loc182))
#loc198 = loc(callsite(#loc51 at #loc182))
#loc199 = loc(callsite(#loc52 at #loc182))
#loc200 = loc(callsite(#loc49 at #loc183))
#loc201 = loc(callsite(#loc51 at #loc183))
#loc202 = loc(callsite(#loc52 at #loc183))
#loc203 = loc(callsite(#loc196 at #loc2))
