//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	triton_kernel           // -- Begin function triton_kernel
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
.noreturn;
.global .align 1 .b8 assertFunc_0[8] = {117, 110, 107, 110, 111, 119, 110};
.global .align 1 .b8 assertFile_0[8] = {117, 110, 107, 110, 111, 119, 110};
.global .align 1 .b8 assertMessage_0[36] = {105, 110, 100, 101, 120, 32, 111, 117, 116, 32, 111, 102, 32, 98, 111, 117, 110, 100, 115, 58, 32, 48, 32, 60, 61, 32, 116, 109, 112, 52, 57, 32, 60, 32, 57};
                                        // @triton_kernel
.visible .entry triton_kernel(
	.param .u64 .ptr .global .align 1 triton_kernel_param_0,
	.param .u64 .ptr .global .align 1 triton_kernel_param_1,
	.param .u64 .ptr .global .align 1 triton_kernel_param_2,
	.param .u32 triton_kernel_param_3,
	.param .u64 .ptr .global .align 1 triton_kernel_param_4,
	.param .u64 .ptr .global .align 1 triton_kernel_param_5
)
.reqntid 32
{
	.reg .pred 	%p<46>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<200>;
	.reg .b64 	%rd<24>;

// %bb.0:
	ld.param.b64 	%rd10, [triton_kernel_param_0];
	ld.param.b64 	%rd11, [triton_kernel_param_1];
	mov.u32 	%r28, %ctaid.x;
	shl.b32 	%r29, %r28, 6;
	mov.u32 	%r30, %tid.x;
	shl.b32 	%r31, %r30, 1;
	and.b32 	%r32, %r31, 62;
	or.b32 	%r1, %r32, %r29;
	setp.gt.s32 	%p9, %r1, 35;
	setp.lt.s32 	%p8, %r1, 36;
	shr.s32 	%r2, %r1, 1;
	mul.hi.s32 	%r33, %r2, 715827883;
	shr.u32 	%r34, %r33, 31;
	add.s32 	%r35, %r33, %r34;
	mul.lo.s32 	%r36, %r35, 6;
	sub.s32 	%r37, %r2, %r36;
	mul.hi.s32 	%r38, %r1, 715827883;
	shr.u32 	%r39, %r38, 31;
	shr.s32 	%r40, %r38, 1;
	add.s32 	%r41, %r40, %r39;
	setp.gt.s32 	%p10, %r37, 4;
	mul.hi.u32 	%r42, %r1, -1431655765;
	shr.u32 	%r43, %r42, 4;
	setp.gt.s32 	%p11, %r1, 23;
	selp.b32 	%r3, %r43, 0, %p11;
	add.s32 	%r44, %r41, 1;
	shr.u32 	%r45, %r44, 31;
	add.s32 	%r46, %r44, %r45;
	shr.s32 	%r47, %r46, 1;
	add.s32 	%r48, %r47, 1;
	setp.gt.s32 	%p12, %r1, 11;
	selp.b32 	%r49, 2, 0, %p12;
	setp.lt.s32 	%p13, %r1, 12;
	selp.b32 	%r50, %r48, 0, %p13;
	add.s32 	%r4, %r50, %r49;
	add.s32 	%r51, %r4, -1;
	min.s32 	%r5, %r3, %r51;
	shl.b32 	%r52, %r5, 3;
	cvt.u16.u32 	%rs11, %r37;
	mov.b32 	%r53, {%rs11, %rs11};
	mov.b32 	%r54, 1;
	add.s16x2 	%r55, %r53, %r54;
	and.b32 	%r56, %r55, 16711935;
	mov.b32 	{%rs12, %rs13}, %r56;
	shr.u16 	%rs14, %rs13, 7;
	shr.u16 	%rs15, %rs12, 7;
	mov.b32 	%r57, {%rs15, %rs14};
	add.s16x2 	%r58, %r55, %r57;
	mov.b32 	{%rs16, %rs17}, %r58;
	cvt.s16.s8 	%rs18, %rs17;
	cvt.s16.s8 	%rs19, %rs16;
	shr.u16 	%rs20, %rs19, 1;
	mov.b32 	%r59, {%rs20, %rs21};
	add.s16x2 	%r60, %r59, %r54;
	mov.b32 	{%rs22, _}, %r60;
	cvt.s16.s8 	%rs23, %rs22;
	shr.s16 	%rs24, %rs18, 1;
	setp.lt.s32 	%p14, %r37, 5;
	setp.gt.s32 	%p15, %r37, 1;
	selp.b16 	%rs2, %rs24, 0, %p15;
	selp.b16 	%rs25, %rs23, 0, %p14;
	mov.b32 	%r61, {%rs25, %rs2};
	selp.b16 	%rs26, 4, 0, %p10;
	mov.b16 	%rs27, 1;
	mov.b32 	%r62, {%rs26, %rs27};
	add.s16x2 	%r63, %r61, %r62;
	mov.b32 	{%rs1, %rs3}, %r63;
	add.s16 	%rs28, %rs1, -1;
	min.s16 	%rs29, %rs2, %rs28;
	setp.gt.s16 	%p16, %rs28, %rs2;
	selp.b16 	%rs30, %rs3, 0, %p16;
	setp.gt.s16 	%p17, %rs1, %rs3;
	selp.b16 	%rs31, 0, %rs28, %p17;
	add.s16 	%rs32, %rs30, %rs31;
	mul.wide.s16 	%r6, %rs32, 2;
	cvt.s32.s16 	%r64, %rs29;
	mul.wide.s32 	%rd12, %r64, 2;
	cvt.u32.u64 	%r65, %rd12;
	and.b32 	%r7, %r65, -2;
	add.s32 	%r66, %r7, %r52;
	cvt.s64.s32 	%rd13, %r66;
	add.s64 	%rd2, %rd10, %rd13;
	// begin inline asm
	mov.u16 %rs7, 0x0;
	@%p8 ld.global.b16 { %rs7 }, [ %rd2 + 0 ];
	// end inline asm
	mad.wide.s32 	%rd3, %r66, 4, %rd11;
	// begin inline asm
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	@%p8 ld.global.v2.b32 { %r20, %r21 }, [ %rd3 + 0 ];
	// end inline asm
	add.s32 	%r67, %r6, %r52;
	cvt.s64.s32 	%rd14, %r67;
	add.s64 	%rd4, %rd10, %rd14;
	// begin inline asm
	mov.u16 %rs8, 0x0;
	@%p8 ld.global.b16 { %rs8 }, [ %rd4 + 0 ];
	// end inline asm
	mad.wide.s32 	%rd5, %r67, 4, %rd11;
	// begin inline asm
	mov.u32 %r22, 0x0;
	mov.u32 %r23, 0x0;
	@%p8 ld.global.v2.b32 { %r22, %r23 }, [ %rd5 + 0 ];
	// end inline asm
	add.s32 	%r12, %r3, 1;
	setp.lt.s32 	%p18, %r3, %r51;
	selp.b32 	%r68, %r12, 0, %p18;
	setp.gt.s32 	%p19, %r4, %r12;
	selp.b32 	%r69, 0, %r51, %p19;
	add.s32 	%r13, %r68, %r69;
	shl.b32 	%r70, %r13, 3;
	add.s32 	%r71, %r7, %r70;
	cvt.s64.s32 	%rd15, %r71;
	add.s64 	%rd6, %rd10, %rd15;
	// begin inline asm
	mov.u16 %rs9, 0x0;
	@%p8 ld.global.b16 { %rs9 }, [ %rd6 + 0 ];
	// end inline asm
	mad.wide.s32 	%rd7, %r71, 4, %rd11;
	// begin inline asm
	mov.u32 %r24, 0x0;
	mov.u32 %r25, 0x0;
	@%p8 ld.global.v2.b32 { %r24, %r25 }, [ %rd7 + 0 ];
	// end inline asm
	add.s32 	%r72, %r6, %r70;
	cvt.s64.s32 	%rd16, %r72;
	add.s64 	%rd8, %rd10, %rd16;
	// begin inline asm
	mov.u16 %rs10, 0x0;
	@%p8 ld.global.b16 { %rs10 }, [ %rd8 + 0 ];
	// end inline asm
	shr.s16 	%rs33, %rs10, 8;
	cvt.s16.s8 	%rs34, %rs10;
	shr.u16 	%rs35, %rs10, 8;
	mad.wide.s32 	%rd9, %r72, 4, %rd11;
	// begin inline asm
	mov.u32 %r26, 0x0;
	mov.u32 %r27, 0x0;
	@%p8 ld.global.v2.b32 { %r26, %r27 }, [ %rd9 + 0 ];
	// end inline asm
	cvt.u32.u16 	%r73, %rs35;
	cvt.s32.s8 	%r74, %r73;
	cvt.u32.u16 	%r75, %rs10;
	cvt.s32.s8 	%r76, %r75;
	add.s32 	%r77, %r76, 9;
	add.s32 	%r78, %r74, 9;
	setp.lt.s16 	%p20, %rs34, 0;
	setp.lt.s16 	%p21, %rs33, 0;
	selp.b32 	%r79, %r78, %r74, %p21;
	selp.b32 	%r80, %r77, %r76, %p20;
	max.u32 	%r81, %r80, %r79;
	setp.lt.u32 	%p22, %r81, 9;
	or.pred 	%p23, %p9, %p22;
	@%p23 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	ld.param.b64 	%rd1, [triton_kernel_param_2];
	setp.lt.s32 	%p27, %r3, %r4;
	setp.lt.s16 	%p28, %rs2, %rs1;
	and.pred 	%p29, %p19, %p28;
	shr.u16 	%rs36, %rs9, 8;
	and.pred 	%p30, %p27, %p17;
	shr.u16 	%rs37, %rs7, 8;
	cvt.s16.s8 	%rs38, %rs7;
	setp.lt.s16 	%p31, %rs38, 0;
	cvt.s16.s8 	%rs39, %rs8;
	setp.lt.s16 	%p32, %rs39, 0;
	cvt.u32.u16 	%r84, %rs8;
	cvt.s32.s8 	%r85, %r84;
	cvt.u32.u16 	%r86, %rs7;
	cvt.s32.s8 	%r87, %r86;
	add.s32 	%r88, %r87, 9;
	add.s32 	%r89, %r85, 9;
	selp.b32 	%r90, %r89, %r85, %p32;
	selp.b32 	%r91, %r88, %r87, %p31;
	bar.sync 	0;
	mad.lo.s32 	%r92, %r5, 12, -7;
	mad.lo.s32 	%r93, %r13, 12, -7;
	mov.b32 	%r94, {%rs8, %rs40};
	prmt.b32 	%r95, %r94, 0, 0x9991U;
	cvt.u16.u32 	%rs41, %r95;
	setp.lt.s16 	%p33, %rs41, 0;
	cvt.s16.s8 	%rs42, %rs9;
	setp.lt.s16 	%p34, %rs42, 0;
	shr.s16 	%rs43, %rs9, 8;
	setp.lt.s16 	%p35, %rs43, 0;
	shr.s16 	%rs44, %rs7, 8;
	setp.lt.s16 	%p36, %rs44, 0;
	cvt.u32.u16 	%r96, %rs37;
	cvt.s32.s8 	%r97, %r96;
	cvt.u32.u16 	%r98, %rs36;
	cvt.s32.s8 	%r99, %r98;
	cvt.u32.u16 	%r100, %rs9;
	cvt.s32.s8 	%r101, %r100;
	add.s32 	%r102, %r95, 9;
	add.s32 	%r103, %r101, 9;
	add.s32 	%r104, %r99, 9;
	add.s32 	%r105, %r97, 9;
	selp.b32 	%r106, %r102, %r95, %p33;
	selp.b32 	%r107, %r105, %r97, %p36;
	selp.b32 	%r108, %r104, %r99, %p35;
	selp.b32 	%r109, %r103, %r101, %p34;
	mul.hi.s32 	%r114, %r106, 1431655766;
	shr.u32 	%r115, %r114, 31;
	add.s32 	%r116, %r114, %r115;
	mul.lo.s32 	%r117, %r116, 3;
	mul.hi.s32 	%r118, %r107, 1431655766;
	shr.u32 	%r119, %r118, 31;
	add.s32 	%r120, %r118, %r119;
	mul.lo.s32 	%r121, %r120, 3;
	mul.hi.s32 	%r122, %r108, 1431655766;
	shr.u32 	%r123, %r122, 31;
	add.s32 	%r124, %r122, %r123;
	mul.lo.s32 	%r125, %r124, 3;
	mul.hi.s32 	%r126, %r91, 1431655766;
	shr.u32 	%r127, %r126, 31;
	add.s32 	%r128, %r126, %r127;
	mul.lo.s32 	%r129, %r128, 3;
	mul.hi.s32 	%r130, %r90, 1431655766;
	shr.u32 	%r131, %r130, 31;
	add.s32 	%r132, %r130, %r131;
	mul.lo.s32 	%r133, %r132, 3;
	mul.hi.s32 	%r134, %r109, 1431655766;
	shr.u32 	%r135, %r134, 31;
	add.s32 	%r136, %r134, %r135;
	mul.lo.s32 	%r137, %r136, 3;
	mul.hi.s32 	%r138, %r80, 1431655766;
	shr.u32 	%r139, %r138, 31;
	add.s32 	%r140, %r138, %r139;
	mul.lo.s32 	%r141, %r140, 3;
	mul.hi.s32 	%r142, %r79, 1431655766;
	shr.u32 	%r143, %r142, 31;
	add.s32 	%r144, %r142, %r143;
	mul.lo.s32 	%r145, %r144, 3;
	add.s32 	%r146, %r93, %r6;
	add.s32 	%r147, %r92, %r7;
	add.s32 	%r148, %r93, %r7;
	add.s32 	%r149, %r92, %r6;
	shl.b32 	%r150, %r106, 1;
	shl.b32 	%r151, %r109, 1;
	shl.b32 	%r152, %r90, 1;
	shl.b32 	%r153, %r91, 1;
	shl.b32 	%r154, %r108, 1;
	shl.b32 	%r155, %r107, 1;
	shl.b32 	%r156, %r79, 1;
	shl.b32 	%r157, %r80, 1;
	sub.s32 	%r158, %r117, %r106;
	add.s32 	%r159, %r149, %r158;
	sub.s32 	%r160, %r137, %r109;
	add.s32 	%r161, %r148, %r160;
	sub.s32 	%r162, %r133, %r90;
	add.s32 	%r163, %r149, %r162;
	sub.s32 	%r164, %r129, %r91;
	add.s32 	%r165, %r147, %r164;
	sub.s32 	%r166, %r125, %r108;
	add.s32 	%r167, %r148, %r166;
	sub.s32 	%r168, %r121, %r107;
	add.s32 	%r169, %r147, %r168;
	sub.s32 	%r170, %r145, %r79;
	add.s32 	%r171, %r146, %r170;
	sub.s32 	%r172, %r141, %r80;
	add.s32 	%r173, %r146, %r172;
	add.s32 	%r174, %r159, %r150;
	add.s32 	%r175, %r169, %r155;
	add.s32 	%r176, %r167, %r154;
	add.s32 	%r177, %r165, %r153;
	add.s32 	%r178, %r163, %r152;
	add.s32 	%r179, %r161, %r151;
	add.s32 	%r180, %r173, %r157;
	add.s32 	%r181, %r171, %r156;
	setp.eq.b32 	%p37, %r174, %r2;
	setp.eq.b32 	%p38, %r179, %r2;
	setp.eq.b32 	%p39, %r178, %r2;
	setp.eq.b32 	%p40, %r177, %r2;
	setp.eq.b32 	%p41, %r176, %r2;
	setp.eq.b32 	%p42, %r175, %r2;
	setp.eq.b32 	%p43, %r181, %r2;
	setp.eq.b32 	%p44, %r180, %r2;
	selp.f32 	%r182, %r21, 0f00000000, %p42;
	add.f32 	%r183, %r182, %r23;
	selp.f32 	%r184, %r183, %r182, %p37;
	selp.f32 	%r185, %r184, %r182, %p30;
	add.f32 	%r186, %r185, %r25;
	selp.f32 	%r187, %r186, %r185, %p41;
	selp.f32 	%r188, %r187, %r185, %p29;
	selp.f32 	%r189, %r20, 0f00000000, %p40;
	add.f32 	%r190, %r189, %r22;
	selp.f32 	%r191, %r190, %r189, %p39;
	selp.f32 	%r192, %r191, %r189, %p30;
	add.f32 	%r193, %r192, %r24;
	selp.f32 	%r194, %r193, %r192, %p38;
	selp.f32 	%r195, %r194, %r192, %p29;
	and.pred 	%p45, %p19, %p17;
	add.f32 	%r196, %r195, %r26;
	add.f32 	%r197, %r188, %r27;
	selp.f32 	%r198, %r196, %r195, %p44;
	selp.f32 	%r82, %r198, %r195, %p45;
	selp.f32 	%r199, %r197, %r188, %p43;
	selp.f32 	%r83, %r199, %r188, %p45;
	mad.wide.s32 	%rd17, %r1, 4, %rd1;
	// begin inline asm
	@%p8 st.global.v2.b32 [ %rd17 + 0 ], { %r82, %r83 };
	// end inline asm
	ret;
$L__BB0_1:
	{ // callseq 0, 0
	.param .b64 	param0;
	.param .b64 	param1;
	.param .b32 	param2;
	.param .b64 	param3;
	.param .b64 	param4;
	mov.b64 	%rd18, assertFunc_0;
	cvta.global.u64 	%rd19, %rd18;
	st.param.b64 	[param3], %rd19;
	mov.b64 	%rd20, assertFile_0;
	cvta.global.u64 	%rd21, %rd20;
	st.param.b64 	[param1], %rd21;
	mov.b64 	%rd22, assertMessage_0;
	cvta.global.u64 	%rd23, %rd22;
	st.param.b64 	[param0], %rd23;
	st.param.b64 	[param4], 1;
	st.param.b32 	[param2], 0;
	call.uni __assertfail, (param0, param1, param2, param3, param4);
	} // callseq 0
	trap;
                                        // -- End function
}
