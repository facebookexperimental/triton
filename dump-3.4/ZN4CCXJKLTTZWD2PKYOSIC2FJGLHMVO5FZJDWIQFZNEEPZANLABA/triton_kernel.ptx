//
// Generated by LLVM NVPTX Back-End
//

.version 8.7
.target sm_90a
.address_size 64

	// .globl	triton_kernel           // -- Begin function triton_kernel
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
.noreturn;
.global .align 1 .b8 assertFunc_0[8] = {117, 110, 107, 110, 111, 119, 110};
.global .align 1 .b8 assertFile_0[8] = {117, 110, 107, 110, 111, 119, 110};
.global .align 1 .b8 assertMessage_0[36] = {105, 110, 100, 101, 120, 32, 111, 117, 116, 32, 111, 102, 32, 98, 111, 117, 110, 100, 115, 58, 32, 48, 32, 60, 61, 32, 116, 109, 112, 52, 57, 32, 60, 32, 57};
                                        // @triton_kernel
.visible .entry triton_kernel(
	.param .u64 .ptr .global .align 1 triton_kernel_param_0,
	.param .u64 .ptr .global .align 1 triton_kernel_param_1,
	.param .u64 .ptr .global .align 1 triton_kernel_param_2,
	.param .u32 triton_kernel_param_3,
	.param .u64 .ptr .global .align 1 triton_kernel_param_4
)
.reqntid 32
{
	.reg .pred 	%p<46>;
	.reg .b16 	%rs<44>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<29>;

// %bb.0:
	ld.param.b64 	%rd10, [triton_kernel_param_0];
	ld.param.b64 	%rd11, [triton_kernel_param_1];
	mov.u32 	%r28, %ctaid.x;
	shl.b32 	%r29, %r28, 6;
	mov.u32 	%r30, %tid.x;
	shl.b32 	%r31, %r30, 1;
	and.b32 	%r32, %r31, 62;
	or.b32 	%r1, %r32, %r29;
	setp.gt.s32 	%p9, %r1, 35;
	setp.lt.s32 	%p8, %r1, 36;
	shr.s32 	%r2, %r1, 1;
	mul.hi.s32 	%r33, %r2, 715827883;
	shr.u32 	%r34, %r33, 31;
	add.s32 	%r35, %r33, %r34;
	mul.lo.s32 	%r36, %r35, 6;
	sub.s32 	%r37, %r2, %r36;
	mul.hi.s32 	%r38, %r1, 715827883;
	shr.u32 	%r39, %r38, 31;
	shr.s32 	%r40, %r38, 1;
	add.s32 	%r41, %r40, %r39;
	cvt.u16.u32 	%rs11, %r37;
	and.b16 	%rs12, %rs11, 128;
	shr.u16 	%rs13, %rs12, 7;
	add.s16 	%rs14, %rs11, %rs13;
	cvt.s16.s8 	%rs15, %rs14;
	shr.s16 	%rs16, %rs15, 1;
	add.s16 	%rs17, %rs11, 1;
	and.b16 	%rs18, %rs17, 128;
	shr.u16 	%rs19, %rs18, 7;
	add.s16 	%rs20, %rs17, %rs19;
	cvt.s16.s8 	%rs21, %rs20;
	shr.s16 	%rs22, %rs21, 1;
	add.s16 	%rs23, %rs22, 1;
	setp.gt.s32 	%p10, %r37, 4;
	mul.hi.u32 	%r42, %r1, -1431655765;
	shr.u32 	%r43, %r42, 4;
	setp.gt.s32 	%p11, %r1, 23;
	selp.b32 	%r3, %r43, 0, %p11;
	add.s32 	%r44, %r41, 1;
	shr.u32 	%r45, %r44, 31;
	add.s32 	%r46, %r44, %r45;
	shr.s32 	%r47, %r46, 1;
	add.s32 	%r48, %r47, 1;
	setp.gt.s32 	%p12, %r1, 11;
	selp.b32 	%r49, 2, 0, %p12;
	setp.lt.s32 	%p13, %r1, 12;
	selp.b32 	%r50, %r48, 0, %p13;
	add.s32 	%r4, %r50, %r49;
	add.s32 	%r51, %r4, -1;
	min.s32 	%r5, %r3, %r51;
	shl.b32 	%r52, %r5, 3;
	setp.lt.s32 	%p14, %r37, 5;
	setp.gt.s32 	%p15, %r37, 1;
	selp.b16 	%rs2, %rs16, 0, %p15;
	selp.b16 	%rs24, %rs23, 0, %p14;
	mov.b32 	%r53, {%rs24, %rs2};
	selp.b16 	%rs25, 4, 0, %p10;
	mov.b16 	%rs26, 1;
	mov.b32 	%r54, {%rs25, %rs26};
	add.s16x2 	%r55, %r53, %r54;
	mov.b32 	{%rs1, %rs3}, %r55;
	add.s16 	%rs27, %rs1, -1;
	min.s16 	%rs28, %rs2, %rs27;
	setp.gt.s16 	%p16, %rs27, %rs2;
	selp.b16 	%rs29, %rs3, 0, %p16;
	setp.gt.s16 	%p17, %rs1, %rs3;
	selp.b16 	%rs30, 0, %rs27, %p17;
	add.s16 	%rs31, %rs29, %rs30;
	mul.wide.s16 	%r6, %rs31, 2;
	cvt.s32.s16 	%r56, %rs28;
	mul.wide.s32 	%rd12, %r56, 2;
	cvt.u32.u64 	%r57, %rd12;
	and.b32 	%r7, %r57, -2;
	add.s32 	%r58, %r7, %r52;
	cvt.s64.s32 	%rd13, %r58;
	add.s64 	%rd2, %rd10, %rd13;
	// begin inline asm
	mov.u16 %rs7, 0x0;
	@%p8 ld.global.b16 { %rs7 }, [ %rd2 + 0 ];
	// end inline asm
	mul.wide.s32 	%rd14, %r58, 4;
	add.s64 	%rd3, %rd11, %rd14;
	// begin inline asm
	mov.u32 %r20, 0x0;
	mov.u32 %r21, 0x0;
	@%p8 ld.global.v2.b32 { %r20, %r21 }, [ %rd3 + 0 ];
	// end inline asm
	add.s32 	%r59, %r6, %r52;
	cvt.s64.s32 	%rd15, %r59;
	add.s64 	%rd4, %rd10, %rd15;
	// begin inline asm
	mov.u16 %rs8, 0x0;
	@%p8 ld.global.b16 { %rs8 }, [ %rd4 + 0 ];
	// end inline asm
	mul.wide.s32 	%rd16, %r59, 4;
	add.s64 	%rd5, %rd11, %rd16;
	// begin inline asm
	mov.u32 %r22, 0x0;
	mov.u32 %r23, 0x0;
	@%p8 ld.global.v2.b32 { %r22, %r23 }, [ %rd5 + 0 ];
	// end inline asm
	add.s32 	%r12, %r3, 1;
	setp.lt.s32 	%p18, %r3, %r51;
	selp.b32 	%r60, %r12, 0, %p18;
	setp.gt.s32 	%p19, %r4, %r12;
	selp.b32 	%r61, 0, %r51, %p19;
	add.s32 	%r13, %r60, %r61;
	shl.b32 	%r62, %r13, 3;
	add.s32 	%r63, %r7, %r62;
	cvt.s64.s32 	%rd17, %r63;
	add.s64 	%rd6, %rd10, %rd17;
	// begin inline asm
	mov.u16 %rs9, 0x0;
	@%p8 ld.global.b16 { %rs9 }, [ %rd6 + 0 ];
	// end inline asm
	mul.wide.s32 	%rd18, %r63, 4;
	add.s64 	%rd7, %rd11, %rd18;
	// begin inline asm
	mov.u32 %r24, 0x0;
	mov.u32 %r25, 0x0;
	@%p8 ld.global.v2.b32 { %r24, %r25 }, [ %rd7 + 0 ];
	// end inline asm
	add.s32 	%r64, %r6, %r62;
	cvt.s64.s32 	%rd19, %r64;
	add.s64 	%rd8, %rd10, %rd19;
	// begin inline asm
	mov.u16 %rs10, 0x0;
	@%p8 ld.global.b16 { %rs10 }, [ %rd8 + 0 ];
	// end inline asm
	shr.s16 	%rs32, %rs10, 8;
	cvt.s16.s8 	%rs33, %rs10;
	shr.u16 	%rs34, %rs10, 8;
	mul.wide.s32 	%rd20, %r64, 4;
	add.s64 	%rd9, %rd11, %rd20;
	// begin inline asm
	mov.u32 %r26, 0x0;
	mov.u32 %r27, 0x0;
	@%p8 ld.global.v2.b32 { %r26, %r27 }, [ %rd9 + 0 ];
	// end inline asm
	cvt.u32.u16 	%r65, %rs34;
	cvt.s32.s8 	%r66, %r65;
	cvt.u32.u16 	%r67, %rs10;
	cvt.s32.s8 	%r68, %r67;
	add.s32 	%r69, %r68, 9;
	add.s32 	%r70, %r66, 9;
	setp.lt.s16 	%p20, %rs33, 0;
	setp.lt.s16 	%p21, %rs32, 0;
	selp.b32 	%r19, %r70, %r66, %p21;
	selp.b32 	%r71, %r69, %r68, %p20;
	max.u32 	%r73, %r71, %r19;
	setp.lt.u32 	%p22, %r73, 9;
	or.pred 	%p23, %p9, %p22;
	@%p23 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	ld.param.b64 	%rd1, [triton_kernel_param_2];
	setp.lt.s32 	%p27, %r3, %r4;
	setp.lt.s16 	%p28, %rs2, %rs1;
	and.pred 	%p29, %p19, %p28;
	shr.u16 	%rs35, %rs9, 8;
	and.pred 	%p30, %p27, %p17;
	shr.u16 	%rs36, %rs7, 8;
	cvt.s16.s8 	%rs37, %rs8;
	setp.lt.s16 	%p31, %rs37, 0;
	cvt.s16.s8 	%rs38, %rs7;
	setp.lt.s16 	%p32, %rs38, 0;
	cvt.u32.u16 	%r76, %rs7;
	cvt.s32.s8 	%r77, %r76;
	cvt.u32.u16 	%r78, %rs8;
	cvt.s32.s8 	%r79, %r78;
	add.s32 	%r80, %r79, 9;
	add.s32 	%r81, %r77, 9;
	selp.b32 	%r82, %r81, %r77, %p32;
	selp.b32 	%r83, %r80, %r79, %p31;
	bar.sync 	0;
	mad.lo.s32 	%r86, %r5, 12, -7;
	mad.lo.s32 	%r87, %r13, 12, -7;
	mov.b32 	%r88, {%rs8, %rs39};
	bfe.s32 	%r89, %r88, 8, 8;
	cvt.s8.s32 	%rs40, %r89;
	setp.lt.s16 	%p33, %rs40, 0;
	cvt.s16.s8 	%rs41, %rs9;
	setp.lt.s16 	%p34, %rs41, 0;
	shr.s16 	%rs42, %rs9, 8;
	setp.lt.s16 	%p35, %rs42, 0;
	shr.s16 	%rs43, %rs7, 8;
	setp.lt.s16 	%p36, %rs43, 0;
	cvt.u32.u16 	%r90, %rs36;
	cvt.s32.s8 	%r91, %r90;
	cvt.u32.u16 	%r92, %rs35;
	cvt.s32.s8 	%r93, %r92;
	cvt.u32.u16 	%r94, %rs9;
	cvt.s32.s8 	%r95, %r94;
	add.s32 	%r96, %r89, 9;
	add.s32 	%r97, %r95, 9;
	add.s32 	%r98, %r93, 9;
	add.s32 	%r99, %r91, 9;
	selp.b32 	%r100, %r96, %r89, %p33;
	selp.b32 	%r101, %r99, %r91, %p36;
	selp.b32 	%r102, %r98, %r93, %p35;
	selp.b32 	%r103, %r97, %r95, %p34;
	mul.hi.s32 	%r108, %r100, 1431655766;
	shr.u32 	%r109, %r108, 31;
	add.s32 	%r110, %r108, %r109;
	mul.lo.s32 	%r111, %r110, 3;
	mul.hi.s32 	%r112, %r101, 1431655766;
	shr.u32 	%r113, %r112, 31;
	add.s32 	%r114, %r112, %r113;
	mul.lo.s32 	%r115, %r114, 3;
	mul.hi.s32 	%r116, %r102, 1431655766;
	shr.u32 	%r117, %r116, 31;
	add.s32 	%r118, %r116, %r117;
	mul.lo.s32 	%r119, %r118, 3;
	mul.hi.s32 	%r120, %r82, 1431655766;
	shr.u32 	%r121, %r120, 31;
	add.s32 	%r122, %r120, %r121;
	mul.lo.s32 	%r123, %r122, 3;
	mul.hi.s32 	%r124, %r83, 1431655766;
	shr.u32 	%r125, %r124, 31;
	add.s32 	%r126, %r124, %r125;
	mul.lo.s32 	%r127, %r126, 3;
	mul.hi.s32 	%r128, %r103, 1431655766;
	shr.u32 	%r129, %r128, 31;
	add.s32 	%r130, %r128, %r129;
	mul.lo.s32 	%r131, %r130, 3;
	mul.hi.s32 	%r132, %r71, 1431655766;
	shr.u32 	%r133, %r132, 31;
	add.s32 	%r134, %r132, %r133;
	mul.lo.s32 	%r135, %r134, 3;
	mul.hi.s32 	%r136, %r19, 1431655766;
	shr.u32 	%r137, %r136, 31;
	add.s32 	%r138, %r136, %r137;
	mul.lo.s32 	%r139, %r138, 3;
	add.s32 	%r140, %r87, %r6;
	add.s32 	%r141, %r86, %r7;
	add.s32 	%r142, %r87, %r7;
	add.s32 	%r143, %r86, %r6;
	shl.b32 	%r144, %r100, 1;
	shl.b32 	%r145, %r103, 1;
	shl.b32 	%r146, %r83, 1;
	shl.b32 	%r147, %r82, 1;
	shl.b32 	%r148, %r102, 1;
	shl.b32 	%r149, %r101, 1;
	shl.b32 	%r150, %r19, 1;
	shl.b32 	%r151, %r71, 1;
	sub.s32 	%r152, %r111, %r100;
	add.s32 	%r153, %r143, %r152;
	sub.s32 	%r154, %r131, %r103;
	add.s32 	%r155, %r142, %r154;
	sub.s32 	%r156, %r127, %r83;
	add.s32 	%r157, %r143, %r156;
	sub.s32 	%r158, %r123, %r82;
	add.s32 	%r159, %r141, %r158;
	sub.s32 	%r160, %r119, %r102;
	add.s32 	%r161, %r142, %r160;
	sub.s32 	%r162, %r115, %r101;
	add.s32 	%r163, %r141, %r162;
	sub.s32 	%r164, %r139, %r19;
	add.s32 	%r165, %r140, %r164;
	sub.s32 	%r166, %r135, %r71;
	add.s32 	%r167, %r140, %r166;
	add.s32 	%r168, %r153, %r144;
	add.s32 	%r169, %r163, %r149;
	add.s32 	%r170, %r161, %r148;
	add.s32 	%r171, %r159, %r147;
	add.s32 	%r172, %r157, %r146;
	add.s32 	%r173, %r155, %r145;
	add.s32 	%r174, %r167, %r151;
	add.s32 	%r175, %r165, %r150;
	setp.eq.s32 	%p37, %r168, %r2;
	setp.eq.s32 	%p38, %r173, %r2;
	setp.eq.s32 	%p39, %r172, %r2;
	setp.eq.s32 	%p40, %r171, %r2;
	setp.eq.s32 	%p41, %r170, %r2;
	setp.eq.s32 	%p42, %r169, %r2;
	setp.eq.s32 	%p43, %r175, %r2;
	setp.eq.s32 	%p44, %r174, %r2;
	selp.f32 	%r176, %r21, 0f00000000, %p42;
	add.f32 	%r177, %r176, %r23;
	selp.f32 	%r178, %r177, %r176, %p37;
	selp.f32 	%r179, %r178, %r176, %p30;
	add.f32 	%r180, %r179, %r25;
	selp.f32 	%r181, %r180, %r179, %p41;
	selp.f32 	%r182, %r181, %r179, %p29;
	selp.f32 	%r183, %r20, 0f00000000, %p40;
	add.f32 	%r184, %r183, %r22;
	selp.f32 	%r185, %r184, %r183, %p39;
	selp.f32 	%r186, %r185, %r183, %p30;
	add.f32 	%r187, %r186, %r24;
	selp.f32 	%r188, %r187, %r186, %p38;
	selp.f32 	%r189, %r188, %r186, %p29;
	and.pred 	%p45, %p19, %p17;
	add.f32 	%r190, %r189, %r26;
	add.f32 	%r191, %r182, %r27;
	selp.f32 	%r192, %r190, %r189, %p44;
	selp.f32 	%r74, %r192, %r189, %p45;
	selp.f32 	%r193, %r191, %r182, %p43;
	selp.f32 	%r75, %r193, %r182, %p45;
	mul.wide.s32 	%rd22, %r1, 4;
	add.s64 	%rd21, %rd1, %rd22;
	// begin inline asm
	@%p8 st.global.v2.b32 [ %rd21 + 0 ], { %r74, %r75 };
	// end inline asm
	ret;
$L__BB0_1:
	mov.b64 	%rd23, assertMessage_0;
	cvta.global.u64 	%rd24, %rd23;
	mov.b64 	%rd25, assertFile_0;
	cvta.global.u64 	%rd26, %rd25;
	mov.b64 	%rd27, assertFunc_0;
	cvta.global.u64 	%rd28, %rd27;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.b64 	[param0], %rd24;
	.param .b64 param1;
	st.param.b64 	[param1], %rd26;
	.param .b32 param2;
	st.param.b32 	[param2], 0;
	.param .b64 param3;
	st.param.b64 	[param3], %rd28;
	.param .b64 param4;
	st.param.b64 	[param4], 1;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	} // callseq 0
	trap;
                                        // -- End function
}
